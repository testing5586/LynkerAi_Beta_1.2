"""
å¤šæ¨¡å‹ AI è°ƒç”¨æ¨¡å—
æ”¯æŒ ChatGPTã€Geminiã€ChatGLMã€DeepSeek ç­‰å¤šä¸ª AI æä¾›å•†
è‡ªåŠ¨ fallback æœºåˆ¶ç¡®ä¿é«˜å¯ç”¨æ€§
é›†æˆæ€§èƒ½ç›‘æ§å’Œä½¿ç”¨ç»Ÿè®¡
"""

import os
import json
import time
import httpx
from typing import Optional, List, Dict, Tuple

try:
    from ai_usage_logger import log_ai_usage
    LOGGER_AVAILABLE = True
except ImportError:
    LOGGER_AVAILABLE = False
    print("âš ï¸ ai_usage_logger æœªæ‰¾åˆ°ï¼Œæ€§èƒ½ç›‘æ§å°†ä¸å¯ç”¨")

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GLM_API_KEY = os.getenv("GLM_API_KEY")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")


class MultiModelAI:
    """ç»Ÿä¸€çš„å¤šæ¨¡å‹ AI è°ƒç”¨æ¥å£"""
    
    DEFAULT_SYSTEM_PROMPT = "ä½ æ˜¯ Lynker Master AIï¼Œæ“…é•¿å‘½ç†ã€å…«å­—ã€ç´«å¾®æ–—æ•°ä¸é“æ¿ç¥æ•°ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚"
    
    FALLBACK_ORDER = ["chatgpt", "gemini", "glm", "deepseek"]
    
    @staticmethod
    def call_openai(prompt: str, system_prompt: str = None, model: str = "gpt-4o") -> Tuple[Optional[str], Optional[str], Optional[Dict]]:
        """è°ƒç”¨ OpenAI ChatGPT API"""
        if not OPENAI_AVAILABLE:
            return None, "OpenAI åº“æœªå®‰è£…", None
        if not OPENAI_API_KEY:
            return None, "ç¼ºå°‘ OPENAI_API_KEY", None
        
        try:
            client = OpenAI(api_key=OPENAI_API_KEY)
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.6,
                max_tokens=2000
            )
            
            answer = response.choices[0].message.content.strip()
            
            token_usage = {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
            
            return answer, None, token_usage
            
        except Exception as e:
            error_msg = f"OpenAI è°ƒç”¨å¤±è´¥: {str(e)}"
            print(f"âš ï¸ {error_msg}")
            return None, error_msg, None
    
    @staticmethod
    def call_gemini(prompt: str, system_prompt: str = None, model: str = "gemini-1.5-pro") -> Tuple[Optional[str], Optional[str], Optional[Dict]]:
        """è°ƒç”¨ Google Gemini API"""
        if not GEMINI_AVAILABLE:
            return None, "Gemini åº“æœªå®‰è£…", None
        if not GEMINI_API_KEY:
            return None, "ç¼ºå°‘ GEMINI_API_KEY", None
        
        try:
            genai.configure(api_key=GEMINI_API_KEY)
            
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"{system_prompt}\n\n{prompt}"
            
            model_instance = genai.GenerativeModel(model)
            response = model_instance.generate_content(full_prompt)
            
            answer = response.text.strip()
            
            token_usage = None
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                token_usage = {
                    "prompt_tokens": response.usage_metadata.prompt_token_count,
                    "completion_tokens": response.usage_metadata.candidates_token_count,
                    "total_tokens": response.usage_metadata.total_token_count
                }
            
            return answer, None, token_usage
            
        except Exception as e:
            error_msg = f"Gemini è°ƒç”¨å¤±è´¥: {str(e)}"
            print(f"âš ï¸ {error_msg}")
            return None, error_msg, None
    
    @staticmethod
    def call_glm(prompt: str, system_prompt: str = None, model: str = "glm-4") -> Tuple[Optional[str], Optional[str], Optional[Dict]]:
        """è°ƒç”¨æ™ºè°± ChatGLM API"""
        if not GLM_API_KEY:
            return None, "ç¼ºå°‘ GLM_API_KEY", None
        
        try:
            headers = {
                "Authorization": f"Bearer {GLM_API_KEY}",
                "Content-Type": "application/json"
            }
            
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            payload = {
                "model": model,
                "messages": messages,
                "temperature": 0.6
            }
            
            response = httpx.post(
                "https://open.bigmodel.cn/api/paas/v4/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                answer = data["choices"][0]["message"]["content"].strip()
                
                token_usage = None
                if "usage" in data:
                    token_usage = {
                        "prompt_tokens": data["usage"].get("prompt_tokens"),
                        "completion_tokens": data["usage"].get("completion_tokens"),
                        "total_tokens": data["usage"].get("total_tokens")
                    }
                
                return answer, None, token_usage
            else:
                error_msg = f"GLM API è¿”å›é”™è¯¯: {response.status_code} - {response.text}"
                print(f"âš ï¸ {error_msg}")
                return None, error_msg, None
                
        except Exception as e:
            error_msg = f"GLM è°ƒç”¨å¤±è´¥: {str(e)}"
            print(f"âš ï¸ {error_msg}")
            return None, error_msg, None
    
    @staticmethod
    def call_deepseek(prompt: str, system_prompt: str = None, model: str = "deepseek-chat") -> Tuple[Optional[str], Optional[str], Optional[Dict]]:
        """è°ƒç”¨ DeepSeek API"""
        if not DEEPSEEK_API_KEY:
            return None, "ç¼ºå°‘ DEEPSEEK_API_KEY", None
        
        try:
            headers = {
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                "Content-Type": "application/json"
            }
            
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            payload = {
                "model": model,
                "messages": messages,
                "temperature": 0.6
            }
            
            response = httpx.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                answer = data["choices"][0]["message"]["content"].strip()
                
                token_usage = None
                if "usage" in data:
                    token_usage = {
                        "prompt_tokens": data["usage"].get("prompt_tokens"),
                        "completion_tokens": data["usage"].get("completion_tokens"),
                        "total_tokens": data["usage"].get("total_tokens")
                    }
                
                return answer, None, token_usage
            else:
                error_msg = f"DeepSeek API è¿”å›é”™è¯¯: {response.status_code} - {response.text}"
                print(f"âš ï¸ {error_msg}")
                return None, error_msg, None
                
        except Exception as e:
            error_msg = f"DeepSeek è°ƒç”¨å¤±è´¥: {str(e)}"
            print(f"âš ï¸ {error_msg}")
            return None, error_msg, None
    
    @classmethod
    def call(cls, provider: str, prompt: str, system_prompt: str = None, enable_fallback: bool = True) -> Dict:
        """
        ç»Ÿä¸€çš„å¤šæ¨¡å‹è°ƒç”¨æ¥å£ï¼ˆé›†æˆæ€§èƒ½ç›‘æ§ï¼‰
        
        Args:
            provider: æ¨¡å‹æä¾›å•† (chatgpt/gemini/glm/deepseek)
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            enable_fallback: æ˜¯å¦å¯ç”¨è‡ªåŠ¨ fallback
        
        Returns:
            {
                "success": bool,
                "provider": str,
                "answer": str,
                "error": str,
                "fallback_used": bool,
                "latency": float
            }
        """
        provider = (provider or "chatgpt").lower()
        system_prompt = system_prompt or cls.DEFAULT_SYSTEM_PROMPT
        
        provider_map = {
            "chatgpt": cls.call_openai,
            "gpt": cls.call_openai,
            "openai": cls.call_openai,
            "gemini": cls.call_gemini,
            "google": cls.call_gemini,
            "glm": cls.call_glm,
            "chatglm": cls.call_glm,
            "zhipu": cls.call_glm,
            "deepseek": cls.call_deepseek,
            "ds": cls.call_deepseek
        }
        
        normalized_provider = provider
        if provider not in provider_map:
            normalized_provider = "chatgpt"
        
        start_time = time.time()
        answer, error, token_usage = provider_map[normalized_provider](prompt, system_prompt)
        latency = time.time() - start_time
        
        if answer:
            if LOGGER_AVAILABLE:
                log_ai_usage(normalized_provider, prompt, token_usage, latency, True, None, False)
            
            return {
                "success": True,
                "provider": normalized_provider,
                "answer": answer,
                "error": None,
                "fallback_used": False,
                "latency": round(latency, 3)
            }
        
        if LOGGER_AVAILABLE:
            log_ai_usage(normalized_provider, prompt, token_usage, latency, False, error, False)
        
        if not enable_fallback:
            return {
                "success": False,
                "provider": normalized_provider,
                "answer": None,
                "error": error,
                "fallback_used": False,
                "latency": round(latency, 3)
            }
        
        print(f"ğŸ”„ {normalized_provider} å¤±è´¥ï¼Œå°è¯• fallback...")
        
        for fallback_provider in cls.FALLBACK_ORDER:
            if fallback_provider == normalized_provider:
                continue
            
            if fallback_provider in provider_map:
                start_time = time.time()
                answer, error, token_usage = provider_map[fallback_provider](prompt, system_prompt)
                fallback_latency = time.time() - start_time
                
                if answer:
                    print(f"âœ… Fallback æˆåŠŸï¼Œä½¿ç”¨ {fallback_provider}")
                    
                    if LOGGER_AVAILABLE:
                        log_ai_usage(fallback_provider, prompt, token_usage, fallback_latency, True, None, True)
                    
                    return {
                        "success": True,
                        "provider": fallback_provider,
                        "answer": answer,
                        "error": None,
                        "fallback_used": True,
                        "latency": round(fallback_latency, 3)
                    }
                else:
                    if LOGGER_AVAILABLE:
                        log_ai_usage(fallback_provider, prompt, token_usage, fallback_latency, False, error, True)
        
        total_latency = time.time() - start_time
        return {
            "success": False,
            "provider": normalized_provider,
            "answer": None,
            "error": "æ‰€æœ‰æ¨¡å‹å‡æœªå“åº”",
            "fallback_used": True,
            "latency": round(total_latency, 3)
        }
    
    @classmethod
    def get_available_providers(cls) -> List[Dict]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹æä¾›å•†"""
        providers = []
        
        if OPENAI_API_KEY and OPENAI_AVAILABLE:
            providers.append({"id": "chatgpt", "name": "ChatGPT (OpenAI)", "available": True})
        else:
            providers.append({"id": "chatgpt", "name": "ChatGPT (OpenAI)", "available": False})
        
        if GEMINI_API_KEY and GEMINI_AVAILABLE:
            providers.append({"id": "gemini", "name": "Gemini (Google)", "available": True})
        else:
            providers.append({"id": "gemini", "name": "Gemini (Google)", "available": False})
        
        if GLM_API_KEY:
            providers.append({"id": "glm", "name": "ChatGLM (æ™ºè°±AI)", "available": True})
        else:
            providers.append({"id": "glm", "name": "ChatGLM (æ™ºè°±AI)", "available": False})
        
        if DEEPSEEK_API_KEY:
            providers.append({"id": "deepseek", "name": "DeepSeek", "available": True})
        else:
            providers.append({"id": "deepseek", "name": "DeepSeek", "available": False})
        
        return providers


if __name__ == "__main__":
    print("ğŸ” æ£€æŸ¥å¯ç”¨çš„ AI æ¨¡å‹æä¾›å•†ï¼š\n")
    providers = MultiModelAI.get_available_providers()
    for p in providers:
        status = "âœ…" if p["available"] else "âŒ"
        print(f"{status} {p['name']}")
    
    print("\n" + "="*60)
    print("æµ‹è¯•è°ƒç”¨ï¼ˆå¦‚æœæœ‰å¯ç”¨çš„ API Keyï¼‰:\n")
    
    test_prompt = "ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯å…«å­—å‘½ç†å­¦ï¼Ÿ"
    result = MultiModelAI.call("chatgpt", test_prompt, enable_fallback=True)
    
    if result["success"]:
        print(f"âœ… è°ƒç”¨æˆåŠŸ")
        print(f"ğŸ“ ä½¿ç”¨æ¨¡å‹: {result['provider']}")
        print(f"ğŸ”„ Fallback: {result['fallback_used']}")
        print(f"ğŸ’¬ å›ç­”: {result['answer'][:100]}...")
    else:
        print(f"âŒ è°ƒç”¨å¤±è´¥: {result['error']}")
