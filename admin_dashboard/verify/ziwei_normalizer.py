# -*- coding: utf-8 -*-
"""
ğŸ”§ LynkerAI ç´«å¾®æ–—æ•°éªŒè¯ç³»ç»Ÿ - Normalizer v1.1
Layer 2: å°† Vision å±‚çš„åŸå§‹ OCR è¾“å‡ºæ ‡å‡†åŒ–ä¸º ZiweiAI_v1.1 ç»“æ„
"""

from datetime import datetime
from typing import Union, Dict, Any
import re


def _empty_v11():
    """
    è¿”å›ç©ºçš„ ZiweiAI_v1.1 æ ‡å‡†ç»“æ„
    """
    return {
        "meta": {
            "parser_version": "ZiweiAI_v1.1",
            "source": "",
            "system": "LynkerAI ZiweiAI",
            "timestamp": datetime.now().isoformat()
        },
        "basic_info": {
            "æ€§åˆ«": "",
            "çœŸå¤ªé˜³æ—¶": "",
            "é˜³å†æ—¥æœŸ": "",
            "é˜´å†æ—¥æœŸ": "",
            "å‘½å±€": "",
            "å‘½ä¸»": "",
            "èº«ä¸»": "",
            "å‡ºç”Ÿåœ°": ""
        },
        "star_map": {},
        "transformations": {
            "ç”Ÿå¹´å››åŒ–": {"ç¦„": "", "æƒ": "", "ç§‘": "", "å¿Œ": ""},
            "æµå¹´å››åŒ–": {"ç¦„": "", "æƒ": "", "ç§‘": "", "å¿Œ": ""}
        },
        "tags": {
            "æ ¼å±€": [],
            "æ€§æ ¼": [],
            "ä¼˜åŠ¿": [],
            "é£é™©å› å­": []
        },
        "astro_fingerprint": [],
        "relationship_vector": {},
        "environment": {},
        "risk": {}
    }


def re_search(pattern, text):
    """
    å®‰å…¨çš„æ­£åˆ™æœç´¢è¾…åŠ©å‡½æ•°
    """
    m = re.search(pattern, text)
    return m.group(1).strip() if m else None


def parse_wenmo_text(text: str) -> Dict[str, Any]:
    """
    ç²¾ç¡®è§£ææ–‡å¢¨å¤©æœºå¯¼å‡ºçš„ .txt ç´«å¾®å‘½ç›˜ï¼ˆv1.3 æ›´æ–°ç‰ˆï¼‰
    """
    import re, json

    out = {
        "meta": {"parser_version": "ZiweiAI_v1.3"},
        "basic_info": {},
        "star_map": {},
        "transformations": {},
        "tags": {}
    }

    # ğŸ”¹ æå–åŸºæœ¬ä¿¡æ¯
    basic_section = re.search(r"åŸºæœ¬ä¿¡æ¯([\s\S]+?)å‘½ç›¤åäºŒå®®", text)
    if basic_section:
        for line in basic_section.group(1).splitlines():
            if ":" in line:
                k, v = line.split(":", 1)
                out["basic_info"][k.strip().replace("â”‚", "").replace("â”œ", "").replace("â””", "")] = v.strip()

    # ğŸ”¹ åŒ¹é…å‘½ç›¤åäºŒå®®åŒºå—
    palace_blocks = re.findall(
        r"â”œ[^\n]*?([å‘½å…„å¤«å­è´¢ç–¾è¿å‹å®˜ç”°ç¦çˆ¶][^å®«å®®]*[å®«å®®])\[[^\]]+\][\s\S]*?(?=\nâ”‚\s*â”œ|\nâ””|\Z)",
        text
    )

    # ğŸ”¹ å¯¹æ¯ä¸ªå®«ä½å†æŠ½å–ä¸»æ˜Ÿã€è¾…æ˜Ÿã€å°æ˜Ÿã€åŒ–æ˜Ÿ
    for block in re.finditer(
        r"â”œ\s*([å‘½å…„å¤«å­è´¢ç–¾è¿å‹å®˜ç”°ç¦çˆ¶][^å®«å®®]*[å®«å®®])\[[^\]]+\]([\s\S]*?)(?=\nâ”‚\s*â”œ|\nâ””|\Z)",
        text
    ):
        palace_name = block.group(1).strip().replace("  ", "").replace("å®®", "å®«")
        content = block.group(2)

        main_star = re.findall(r"ä¸»æ˜Ÿ\s*:\s*([^\n]+)", content)
        sub_star = re.findall(r"è¼”æ˜Ÿ\s*:\s*([^\n]+)", content)
        minor_star = re.findall(r"å°æ˜Ÿ\s*:\s*([^\n]+)", content)
        trans_star = re.findall(r"ç¥ç…[\s\S]*?(?=\nâ”‚\s*â”œ|\nâ””|\Z)", content)

        def clean(s):
            return re.sub(r"\[[^\]]*\]", "", s).replace("ï¼Œ", ",").strip()

        out["star_map"][palace_name] = {
            "ä¸»æ˜Ÿ": clean(main_star[0]) if main_star else "",
            "è¾…æ˜Ÿ": clean(sub_star[0]) if sub_star else "",
            "å°æ˜Ÿ": clean(minor_star[0]) if minor_star else "",
            "åŒ–æ˜Ÿ": ""
        }

    print(f"[Ziwei Parser Debug] æ‰¾åˆ° {len(out['star_map'])} ä¸ªå®«ä½: {list(out['star_map'].keys())}")
    return out


def normalize_ziwei(raw_json: Union[str, Dict[str, Any]], user_profile=None):
    """
    ç¬¬2å±‚ï¼šå°† Vision å±‚ OCR è¾“å‡ºæ ‡å‡†åŒ–ä¸º ZiweiAI_v1.1 ç»“æ„
    
    å‚æ•°:
        raw_json: str/dict, å¯ä»¥æ˜¯æ–‡å¢¨å¤©æœº .txt æ–‡æœ¬ã€Vision Agent è¾“å‡ºæˆ– Parser å±‚è¾“å‡º
        user_profile: dict, å¯é€‰çš„ç”¨æˆ·èµ„æ–™ï¼ˆç”¨äºè‡ªåŠ¨è¡¥å…¨ï¼‰
        
    è¿”å›:
        dict: æ ‡å‡†åŒ–çš„ç´«å¾®å‘½ç›˜ JSON ç»“æ„ï¼ˆv1.1ï¼‰
    """
    
    print(f"[Normalizer v1.1] å¼€å§‹æ ‡å‡†åŒ–å¤„ç†...")
    print(f"[Normalizer Debug] æ”¶åˆ°æ•°æ®ç±»å‹: {type(raw_json)}")
    
    # ğŸ”§ Step 1: å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼ˆ.txt æ–‡æœ¬æ ¼å¼ï¼‰ï¼Œå…ˆè§£æä¸ºå­—å…¸
    if isinstance(raw_json, str):
        print("[Normalizer Debug] æ£€æµ‹åˆ°å­—ç¬¦ä¸²è¾“å…¥ï¼Œå°è¯•è§£ææ–‡å¢¨å¤©æœºæ–‡æœ¬æ ¼å¼")
        if len(raw_json.strip()) < 10:
            print("[Normalizer] âš ï¸ æ–‡æœ¬å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•è§£æ")
            return _empty_v11()
        
        # è°ƒç”¨æ–‡æœ¬è§£æå™¨
        parsed_data = parse_wenmo_text(raw_json)
        if not parsed_data or not parsed_data.get("meta"):
            print("[Normalizer] âš ï¸ æ–‡æœ¬è§£æå¤±è´¥ï¼Œè¿”å›ç©ºç»“æ„")
            return _empty_v11()
        
        # æ–‡æœ¬è§£æå™¨å·²è¿”å›å®Œæ•´ç»“æ„ï¼Œç›´æ¥è¿”å›
        print("[Normalizer Debug] æ–‡æœ¬è§£ææˆåŠŸï¼Œç›´æ¥è¿”å›è§£æç»“æœ")
        return parsed_data
    
    # ğŸ”§ Step 2: å¦‚æœä¸æ˜¯å­—å…¸ï¼Œè¿”å›ç©ºç»“æ„
    if not isinstance(raw_json, dict):
        print(f"[Normalizer] âš ï¸ è¾“å…¥ç±»å‹ä¸æ”¯æŒ: {type(raw_json)}ï¼Œè¿”å›ç©ºç»“æ„")
        return _empty_v11()
    
    print(f"[Normalizer Debug] æ•°æ®é¡¶å±‚å­—æ®µ: {list(raw_json.keys())}")
    
    # ğŸ”§ Step 3: å…¼å®¹ Parser å±‚ç›´æ¥è¾“å‡ºï¼ˆå·²ç»æ˜¯æ ‡å‡†ç»“æ„ï¼Œä¸éœ€è¦å†åŒ…è£…ï¼‰
    if raw_json.get("meta") and raw_json.get("basic_info"):
        print("[Normalizer Debug] æ£€æµ‹åˆ° Parser å±‚ç›´æ¥è¾“å‡ºï¼Œè·³è¿‡æ•°æ®æå–")
        data = raw_json
    else:
        # æå–åŸå§‹æ•°æ®ï¼ˆVision å±‚è¾“å‡ºï¼‰
        if not raw_json.get("success"):
            print(f"[Normalizer] âŒ Vision å±‚è¯†åˆ«å¤±è´¥")
            return {
                "success": False,
                "error": raw_json.get("error", "è¯†åˆ«å¤±è´¥")
            }
        data = raw_json.get("data", {})
    
    print(f"[Normalizer] æ”¶åˆ°æ•°æ®å­—æ®µ: {list(data.keys())}")
    
    # æå– metaï¼ˆv1.1 ç‰ˆæœ¬æ ‡è¯†ï¼‰
    meta = data.get("meta", {})
    if not meta:
        meta = {
            "parser_version": "ZiweiAI_v1.1",
            "source": "æ–‡å¢¨å¤©æœº",
            "system": "LynkerAI ZiweiAI",
            "timestamp": datetime.now().isoformat(),
            "ocr_timestamp": raw_json.get("timestamp")
        }
    else:
        meta["parser_version"] = "ZiweiAI_v1.1"
        meta["system"] = "LynkerAI ZiweiAI"
    
    # æå–åŸºæœ¬ä¿¡æ¯ï¼ˆæ”¯æŒä¸­è‹±æ–‡é”®åï¼‰
    basic_info_raw = data.get("basic_info", {})
    normalized_basic_info = {
        "æ€§åˆ«": basic_info_raw.get("æ€§åˆ«") or basic_info_raw.get("gender", ""),
        "çœŸå¤ªé˜³æ—¶": basic_info_raw.get("çœŸå¤ªé˜³æ—¶") or basic_info_raw.get("true_solar_time", ""),
        "é˜³å†æ—¥æœŸ": basic_info_raw.get("é˜³å†æ—¥æœŸ") or basic_info_raw.get("solar_date", ""),
        "é˜´å†æ—¥æœŸ": basic_info_raw.get("é˜´å†æ—¥æœŸ") or basic_info_raw.get("lunar_date", ""),
        "å‘½å±€": basic_info_raw.get("å‘½å±€") or basic_info_raw.get("life_bureau", ""),
        "å‘½ä¸»": basic_info_raw.get("å‘½ä¸»") or basic_info_raw.get("destiny_master", ""),
        "èº«ä¸»": basic_info_raw.get("èº«ä¸»") or basic_info_raw.get("body_master", ""),
        "å‡ºç”Ÿåœ°": basic_info_raw.get("å‡ºç”Ÿåœ°") or basic_info_raw.get("birthplace", "")
    }
    
    # è‡ªåŠ¨è¡¥å…¨ï¼šå¦‚æœç¼ºå°‘æ€§åˆ«ä¸”æœ‰ç”¨æˆ·èµ„æ–™
    if not normalized_basic_info["æ€§åˆ«"] and user_profile:
        if user_profile.get("gender"):
            normalized_basic_info["æ€§åˆ«"] = user_profile["gender"]
            print(f"[Normalizer] è‡ªåŠ¨è¡¥å…¨æ€§åˆ«: {user_profile['gender']}")
    
    # æ ‡å‡†åŒ–æ˜Ÿæ›œåˆ†å¸ƒï¼ˆåäºŒå®«ï¼‰
    star_map_raw = data.get("star_map", {})
    clean_star_map = {}
    
    # å®šä¹‰æ ‡å‡†åäºŒå®«é¡ºåº
    standard_palaces = [
        "å‘½å®«", "å…„å¼Ÿå®«", "å¤«å¦»å®«", "å­å¥³å®«", "è´¢å¸›å®«", "ç–¾å„å®«",
        "è¿ç§»å®«", "äº¤å‹å®«", "å®˜ç¦„å®«", "ç”°å®…å®«", "ç¦å¾·å®«", "çˆ¶æ¯å®«"
    ]
    
    for palace in standard_palaces:
        stars = star_map_raw.get(palace, {})
        
        # âœ… v1.3 ä¿®å¤ï¼šå¦‚æœå·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼ˆParser v1.3 è¾“å‡ºï¼‰ï¼Œç›´æ¥ä¿ç•™
        if isinstance(stars, dict):
            clean_star_map[palace] = stars
            continue
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ†å‰²æˆåˆ—è¡¨
        if isinstance(stars, str):
            stars = re.split(r"[ï¼Œã€\s]+", stars)
        
        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²ï¼ˆå¤„ç†æ—§æ ¼å¼çš„åˆ—è¡¨æ•°æ®ï¼‰
        clean_stars = [s.strip() for s in stars if s.strip()]
        clean_star_map[palace] = clean_stars
    
    print(f"[Normalizer] å·²æ ‡å‡†åŒ– {len(clean_star_map)} ä¸ªå®«ä½")
    
    # æ ‡å‡†åŒ–å››åŒ–ä¿¡æ¯ï¼ˆv1.1 åµŒå¥—ç»“æ„ï¼šç”Ÿå¹´å››åŒ– + æµå¹´å››åŒ–ï¼‰
    transformations_raw = data.get("transformations", {})
    normalized_transformations = _normalize_transformations(transformations_raw)
    
    # æå–æ ‡ç­¾ï¼ˆv1.1 åˆ†ç±»ç»“æ„ï¼‰
    tags_raw = data.get("tags", [])
    normalized_tags = _normalize_tags(tags_raw, clean_star_map, normalized_transformations)
    
    # æå–é£é™©è¯„ä¼°
    risk = data.get("risk", {})
    
    # âœ¨ v1.1 æ–°å¢ï¼šç”Ÿæˆæ˜Ÿç›˜æŒ‡çº¹ï¼ˆastro_fingerprintï¼‰
    astro_fingerprint = _generate_astro_fingerprint(
        clean_star_map, 
        normalized_transformations, 
        normalized_basic_info
    )
    
    # âœ¨ v1.1 æ–°å¢ï¼šç”Ÿæˆå…³ç³»å‘é‡ï¼ˆrelationship_vectorï¼‰
    relationship_vector = _generate_relationship_vector(
        clean_star_map, 
        normalized_transformations,
        normalized_tags
    )
    
    # âœ¨ v1.1 æ–°å¢ï¼šç¯å¢ƒä¿¡æ¯ï¼ˆenvironmentï¼‰
    environment = _normalize_environment(data.get("environment", {}), user_profile)
    
    # âœ¨ v4.0 æ–°å¢ï¼šä¿ç•™å¤§é™/å°é™/æµå¹´æ•°æ®ï¼ˆæ¥è‡ª TXT Patch v4.0ï¼‰
    da_xian = data.get("å¤§é™", [])
    xiao_xian = data.get("å°é™", [])
    liu_nian = data.get("æµå¹´", [])
    
    print(f"[Normalizer v1.1] âœ… æ ‡å‡†åŒ–å®Œæˆ")
    if da_xian or xiao_xian or liu_nian:
        print(f"[Normalizer v4.0] âœ… ä¿ç•™å¢å¼ºæ•°æ®: å¤§é™={len(da_xian)}æ¡, å°é™={len(xiao_xian)}æ¡, æµå¹´={len(liu_nian)}æ¡")
    
    # è¿”å›æ ‡å‡†åŒ–ç»“æœï¼ˆv1.1 å®Œæ•´ç»“æ„ + v4.0 å¢å¼ºï¼‰
    result = {
        "success": True,
        "meta": meta,
        "basic_info": normalized_basic_info,
        "astro_fingerprint": astro_fingerprint,
        "star_map": clean_star_map,
        "transformations": normalized_transformations,
        "tags": normalized_tags,
        "relationship_vector": relationship_vector,
        "environment": environment,
        "risk": risk
    }
    
    # v4.0 å¢å¼ºå­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if da_xian:
        result["å¤§é™"] = da_xian
    if xiao_xian:
        result["å°é™"] = xiao_xian
    if liu_nian:
        result["æµå¹´"] = liu_nian
    
    return result


def _normalize_transformations(transformations_raw):
    """
    æ ‡å‡†åŒ–å››åŒ–ä¿¡æ¯ä¸º v1.1 åµŒå¥—ç»“æ„
    
    è¿”å›æ ¼å¼:
    {
        "ç”Ÿå¹´å››åŒ–": {"ç¦„": "å¤©æœº", "æƒ": "å¤©æ¢", "ç§‘": "ç´«å¾®", "å¿Œ": "æ–‡æ›²"},
        "æµå¹´å››åŒ–": {"ç¦„": "æ­¦æ›²", "æƒ": "å¤©åºœ", "ç§‘": "å¤ªé˜³", "å¿Œ": "å·¨é—¨"}
    }
    """
    result = {
        "ç”Ÿå¹´å››åŒ–": {"ç¦„": "", "æƒ": "", "ç§‘": "", "å¿Œ": ""},
        "æµå¹´å››åŒ–": {"ç¦„": "", "æƒ": "", "ç§‘": "", "å¿Œ": ""}
    }
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯åµŒå¥—æ ¼å¼
    if "ç”Ÿå¹´å››åŒ–" in transformations_raw:
        sheng_nian = transformations_raw.get("ç”Ÿå¹´å››åŒ–", {})
        result["ç”Ÿå¹´å››åŒ–"] = {
            "ç¦„": sheng_nian.get("ç¦„", ""),
            "æƒ": sheng_nian.get("æƒ", ""),
            "ç§‘": sheng_nian.get("ç§‘", ""),
            "å¿Œ": sheng_nian.get("å¿Œ", "")
        }
    else:
        # å¹³çš„æ ¼å¼ï¼Œé»˜è®¤ä¸ºç”Ÿå¹´å››åŒ–
        result["ç”Ÿå¹´å››åŒ–"] = {
            "ç¦„": transformations_raw.get("åŒ–ç¦„", ""),
            "æƒ": transformations_raw.get("åŒ–æƒ", ""),
            "ç§‘": transformations_raw.get("åŒ–ç§‘", ""),
            "å¿Œ": transformations_raw.get("åŒ–å¿Œ", "")
        }
    
    # æå–æµå¹´å››åŒ–ï¼ˆå¦‚æœæœ‰ï¼‰
    if "æµå¹´å››åŒ–" in transformations_raw:
        liu_nian = transformations_raw.get("æµå¹´å››åŒ–", {})
        result["æµå¹´å››åŒ–"] = {
            "ç¦„": liu_nian.get("ç¦„", ""),
            "æƒ": liu_nian.get("æƒ", ""),
            "ç§‘": liu_nian.get("ç§‘", ""),
            "å¿Œ": liu_nian.get("å¿Œ", "")
        }
    
    return result


def _normalize_tags(tags_raw, star_map, transformations):
    """
    æ ‡å‡†åŒ–æ ‡ç­¾ä¸º v1.1 åˆ†ç±»ç»“æ„
    
    è¿”å›æ ¼å¼:
    {
        "æ ¼å±€": ["å¤©åºœåå‘½æ ¼", "ç¦„æƒåŒç¾"],
        "æ€§æ ¼": ["ç¨³é‡", "è°¨æ…", "ç†æ€§"],
        "ä¼˜åŠ¿": ["ç®¡ç†åŠ›å¼º", "è´¢åŠ¡æ€ç»´ä½³"],
        "é£é™©å› å­": ["è¿ç§»å®«åŒ–å¿Œ", "å¤«å¦»å®«å†²åŠ¨"]
    }
    """
    result = {
        "æ ¼å±€": [],
        "æ€§æ ¼": [],
        "ä¼˜åŠ¿": [],
        "é£é™©å› å­": []
    }
    
    # å¦‚æœå·²ç»æ˜¯åˆ†ç±»ç»“æ„
    if isinstance(tags_raw, dict):
        for key in ["æ ¼å±€", "æ€§æ ¼", "ä¼˜åŠ¿", "é£é™©å› å­"]:
            if key in tags_raw:
                result[key] = tags_raw[key] if isinstance(tags_raw[key], list) else [tags_raw[key]]
    elif isinstance(tags_raw, list):
        # ç®€å•åˆ—è¡¨ï¼Œé»˜è®¤å½’å…¥æ ¼å±€
        result["æ ¼å±€"] = tags_raw
    
    # è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾
    auto_tags = _generate_auto_tags(star_map, transformations)
    
    # åˆå¹¶è‡ªåŠ¨æ ‡ç­¾
    for category, tag_list in auto_tags.items():
        result[category].extend(tag_list)
        result[category] = list(set(result[category]))  # å»é‡
    
    return result


def _generate_auto_tags(star_map, transformations):
    """
    æ ¹æ®æ˜Ÿæ›œåˆ†å¸ƒè‡ªåŠ¨ç”Ÿæˆåˆ†ç±»æ ‡ç­¾
    """
    tags = {
        "æ ¼å±€": [],
        "æ€§æ ¼": [],
        "ä¼˜åŠ¿": [],
        "é£é™©å› å­": []
    }
    
    ming_gong = star_map.get("å‘½å®«", [])
    
    # æ ¼å±€è¯†åˆ«
    if "ç´«å¾®" in ming_gong:
        tags["æ ¼å±€"].append("ç´«å¾®åå‘½æ ¼")
        tags["æ€§æ ¼"].extend(["é¢†å¯¼åŠ›å¼º", "è‡ªä¿¡"])
    if "å¤©åºœ" in ming_gong:
        tags["æ ¼å±€"].append("å¤©åºœåå‘½æ ¼")
        tags["æ€§æ ¼"].extend(["ç¨³é‡", "è°¨æ…"])
    if "æ­¦æ›²" in ming_gong:
        tags["ä¼˜åŠ¿"].append("è´¢åŠ¡æ€ç»´ä½³")
    
    # å››åŒ–ç»„åˆ
    sheng_nian = transformations.get("ç”Ÿå¹´å››åŒ–", {})
    if sheng_nian.get("ç¦„") and sheng_nian.get("æƒ"):
        tags["æ ¼å±€"].append("ç¦„æƒåŒç¾")
    
    # é£é™©å› å­æ£€æµ‹
    if "åŒ–å¿Œ" in str(star_map.get("è¿ç§»å®«", [])):
        tags["é£é™©å› å­"].append("è¿ç§»å®«åŒ–å¿Œ")
    if "ç ´å†›" in star_map.get("å¤«å¦»å®«", []):
        tags["é£é™©å› å­"].append("å¤«å¦»å®«ç ´å†›")
    
    return tags


def _generate_astro_fingerprint(star_map, transformations, basic_info):
    """
    ç”Ÿæˆæ˜Ÿç›˜æŒ‡çº¹ï¼ˆv1.1 æ–°å¢ï¼‰
    
    è¿”å›:
    {
        "ä¸»æ˜Ÿç»„åˆç¼–ç ": "å¤©åºœ-å¤©æ¢-æ­¦æ›²",
        "åŒ–æ˜Ÿç»„åˆç¼–ç ": "ç¦„æƒç§‘å¿Œ=å¤©æœº-å¤©æ¢-ç´«å¾®-æ–‡æ›²",
        "å±€æ•°ç¼–ç ": "é‡‘å››å±€",
        "ä¸»å®«åœ°æ”¯": "å·³",
        "æ˜Ÿæ›œçŸ©é˜µ": [["å‘½å®«", "å¤©åºœ"], ["å¤«å¦»å®«", "å»‰è´ã€ç ´å†›"], ...]
    }
    """
    ming_gong_stars = star_map.get("å‘½å®«", [])
    
    # ä¸»æ˜Ÿç»„åˆç¼–ç ï¼ˆå–å‘½å®«å‰3é¢—ä¸»æ˜Ÿï¼‰
    main_stars = [s for s in ming_gong_stars if s in [
        "ç´«å¾®", "å¤©åºœ", "æ­¦æ›²", "å¤©ç›¸", "å¤ªé˜³", "å¤ªé˜´", 
        "è´ªç‹¼", "å·¨é—¨", "å¤©æœº", "å¤©æ¢", "ä¸ƒæ€", "ç ´å†›", "å»‰è´", "å¤©åŒ"
    ]][:3]
    main_combo = "-".join(main_stars) if main_stars else "æ— ä¸»æ˜Ÿ"
    
    # åŒ–æ˜Ÿç»„åˆç¼–ç 
    sheng_nian = transformations.get("ç”Ÿå¹´å››åŒ–", {})
    hua_combo = f"ç¦„æƒç§‘å¿Œ={sheng_nian.get('ç¦„', '')}-{sheng_nian.get('æƒ', '')}-{sheng_nian.get('ç§‘', '')}-{sheng_nian.get('å¿Œ', '')}"
    
    # å±€æ•°ç¼–ç 
    ju_shu = basic_info.get("å‘½å±€", "")
    
    # æ˜Ÿæ›œçŸ©é˜µï¼ˆé‡è¦å®«ä½çš„æ˜Ÿæ›œåˆ—è¡¨ï¼‰
    important_palaces = ["å‘½å®«", "å¤«å¦»å®«", "è¿ç§»å®«", "è´¢å¸›å®«", "å®˜ç¦„å®«"]
    star_matrix = []
    for palace in important_palaces:
        stars = star_map.get(palace, [])
        if stars:
            star_matrix.append([palace, "ã€".join(stars)])
    
    return {
        "ä¸»æ˜Ÿç»„åˆç¼–ç ": main_combo,
        "åŒ–æ˜Ÿç»„åˆç¼–ç ": hua_combo,
        "å±€æ•°ç¼–ç ": ju_shu,
        "ä¸»å®«åœ°æ”¯": _extract_dizhi(basic_info),
        "æ˜Ÿæ›œçŸ©é˜µ": star_matrix
    }


def _extract_dizhi(basic_info):
    """
    ä»å‘½å±€ä¿¡æ¯ä¸­æå–åœ°æ”¯ï¼ˆç®€åŒ–å®ç°ï¼‰
    """
    ju_shu = basic_info.get("å‘½å±€", "")
    dizhi_map = {
        "å­": "å­", "ä¸‘": "ä¸‘", "å¯…": "å¯…", "å¯": "å¯",
        "è¾°": "è¾°", "å·³": "å·³", "åˆ": "åˆ", "æœª": "æœª",
        "ç”³": "ç”³", "é…‰": "é…‰", "æˆŒ": "æˆŒ", "äº¥": "äº¥"
    }
    for dz in dizhi_map:
        if dz in ju_shu:
            return dz
    return ""


def _generate_relationship_vector(star_map, transformations, tags):
    """
    ç”Ÿæˆå…³ç³»å‘é‡ï¼ˆv1.1 æ–°å¢ï¼‰
    
    è¿”å›å››ç»´è¯„åˆ†ï¼ˆ0.0-1.0ï¼‰:
    {
        "å©šå§»": 0.82,
        "äº‹ä¸š": 0.91,
        "å¥åº·": 0.78,
        "äººé™…": 0.74
    }
    """
    # åŸºç¡€åˆ†æ•°
    scores = {
        "å©šå§»": 0.70,
        "äº‹ä¸š": 0.70,
        "å¥åº·": 0.70,
        "äººé™…": 0.70
    }
    
    # å©šå§»è¯„åˆ†é€»è¾‘
    fu_qi_gong = star_map.get("å¤«å¦»å®«", [])
    if "ç´«å¾®" in fu_qi_gong or "å¤©åºœ" in fu_qi_gong:
        scores["å©šå§»"] += 0.15
    if "ç ´å†›" in fu_qi_gong or "ä¸ƒæ€" in fu_qi_gong:
        scores["å©šå§»"] -= 0.10
    
    # äº‹ä¸šè¯„åˆ†é€»è¾‘
    guan_lu_gong = star_map.get("å®˜ç¦„å®«", [])
    if "ç´«å¾®" in guan_lu_gong or "æ­¦æ›²" in guan_lu_gong:
        scores["äº‹ä¸š"] += 0.20
    if tags.get("æ ¼å±€") and "ç¦„æƒåŒç¾" in tags["æ ¼å±€"]:
        scores["äº‹ä¸š"] += 0.10
    
    # å¥åº·è¯„åˆ†é€»è¾‘
    ji_e_gong = star_map.get("ç–¾å„å®«", [])
    if not ji_e_gong or "æ— ä¸»æ˜Ÿ" in str(ji_e_gong):
        scores["å¥åº·"] += 0.10
    
    # äººé™…è¯„åˆ†é€»è¾‘
    jiao_you_gong = star_map.get("äº¤å‹å®«", [])
    if "å¤©åŒ" in jiao_you_gong or "å¤©æ¢" in jiao_you_gong:
        scores["äººé™…"] += 0.15
    
    # é™åˆ¶åœ¨ 0.0-1.0 èŒƒå›´
    for key in scores:
        scores[key] = max(0.0, min(1.0, scores[key]))
        scores[key] = round(scores[key], 2)
    
    return scores


def _normalize_environment(env_raw, user_profile):
    """
    æ ‡å‡†åŒ–ç¯å¢ƒä¿¡æ¯ï¼ˆv1.1 æ–°å¢ï¼‰
    
    è¿”å›:
    {
        "city": "å‰éš†å¡",
        "country": "é©¬æ¥è¥¿äºš",
        "climate_zone": "çƒ­å¸¦",
        "humidity_type": "æ½®æ¹¿",
        "terrain_type": "æ²¿æµ·"
    }
    """
    result = {
        "city": env_raw.get("city", ""),
        "country": env_raw.get("country", ""),
        "climate_zone": env_raw.get("climate_zone", ""),
        "humidity_type": env_raw.get("humidity_type", ""),
        "terrain_type": env_raw.get("terrain_type", "")
    }
    
    # ä»ç”¨æˆ·èµ„æ–™è‡ªåŠ¨è¡¥å…¨
    if user_profile:
        if not result["city"] and user_profile.get("city"):
            result["city"] = user_profile["city"]
        if not result["country"] and user_profile.get("country"):
            result["country"] = user_profile["country"]
    
    return result


def validate_ziwei_structure(normalized_json):
    """
    éªŒè¯æ ‡å‡†åŒ–åçš„ç´«å¾®å‘½ç›˜ç»“æ„æ˜¯å¦å®Œæ•´ï¼ˆv1.1ï¼‰
    
    å‚æ•°:
        normalized_json: dict, æ ‡å‡†åŒ–åçš„ç´«å¾®å‘½ç›˜æ•°æ®
        
    è¿”å›:
        dict: éªŒè¯ç»“æœ {"valid": bool, "errors": list, "warnings": list}
    """
    errors = []
    warnings = []
    
    # æ£€æŸ¥å¿…è¦å­—æ®µï¼ˆv1.1ï¼‰
    required_fields = [
        "meta", "basic_info", "astro_fingerprint", "star_map", 
        "transformations", "tags", "relationship_vector", "environment"
    ]
    for field in required_fields:
        if field not in normalized_json:
            errors.append(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
    
    # æ£€æŸ¥åäºŒå®«æ˜¯å¦å®Œæ•´
    if "star_map" in normalized_json:
        standard_palaces = [
            "å‘½å®«", "å…„å¼Ÿå®«", "å¤«å¦»å®«", "å­å¥³å®«", "è´¢å¸›å®«", "ç–¾å„å®«",
            "è¿ç§»å®«", "äº¤å‹å®«", "å®˜ç¦„å®«", "ç”°å®…å®«", "ç¦å¾·å®«", "çˆ¶æ¯å®«"
        ]
        
        star_map = normalized_json["star_map"]
        for palace in standard_palaces:
            if palace not in star_map:
                warnings.append(f"ç¼ºå°‘å®«ä½: {palace}")
    
    # æ£€æŸ¥å››åŒ–ç»“æ„ï¼ˆv1.1 åµŒå¥—æ ¼å¼ï¼‰
    if "transformations" in normalized_json:
        trans = normalized_json["transformations"]
        if "ç”Ÿå¹´å››åŒ–" not in trans:
            warnings.append("ç¼ºå°‘ç”Ÿå¹´å››åŒ–ä¿¡æ¯")
        else:
            for hua in ["ç¦„", "æƒ", "ç§‘", "å¿Œ"]:
                if not trans["ç”Ÿå¹´å››åŒ–"].get(hua):
                    warnings.append(f"ç”Ÿå¹´å››åŒ–ç¼ºå°‘: {hua}")
    
    # æ£€æŸ¥ v1.1 æ–°å¢å­—æ®µ
    if "astro_fingerprint" in normalized_json:
        fp = normalized_json["astro_fingerprint"]
        if not fp.get("ä¸»æ˜Ÿç»„åˆç¼–ç "):
            warnings.append("æ˜Ÿç›˜æŒ‡çº¹ç¼ºå°‘ä¸»æ˜Ÿç»„åˆç¼–ç ")
    
    return {
        "valid": len(errors) == 0,
        "errors": errors,
        "warnings": warnings
    }


# ===========================================
# Ziwei AI Patch v5.3 - Normalizer é€ä¼ æ‰€æœ‰å­—æ®µ
# ===========================================
def normalize_ziwei_data(data):
    if not isinstance(data, dict):
        return data
    normalized = {}
    for k, v in data.items():
        if isinstance(v, dict):
            normalized[k] = normalize_ziwei_data(v)
        else:
            normalized[k] = v
    return normalized
