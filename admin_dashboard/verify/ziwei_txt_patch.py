# -*- coding: utf-8 -*-
"""
ğŸ§© Ziwei TXT Patch v2.0 - WenMo Tree Format
è§£ææ–‡å¢¨å¤©æœºå¯¼å‡ºçš„æ ‘çŠ¶ç»“æ„ .txt å‘½ç›˜æ–‡ä»¶
"""
import re

palace_hdr_re = re.compile(
    r'^[\sâ”‚â”œâ””]*[â”‚â”œâ””]?\s*(?P<name>[\u4e00-\u9fa5]{1,4}\s*[å®«å®®])\[(?P<dz>[^\]]+)\](?:\[[^\]]+\])?\s*$',
    re.M
)
import json


def parse_wenmo_txt_to_json(txt: str):
    """
    è§£ææ–‡å¢¨å¤©æœºæ ‘çŠ¶æ ¼å¼ TXT å‘½ç›˜æ–‡ä»¶ã€‚
    
    æ ¼å¼ç¤ºä¾‹ï¼š
    â”œè²¡å¸›å®®[æˆŠå­][èº«å®®]
    â”‚ â”‚ â”œä¸»æ˜Ÿ : ç ´è»[å»Ÿ]
    â”‚ â”‚ â”œè¼”æ˜Ÿ : å¤©é­[æ—º]
    â”‚ â”‚ â”œå°æ˜Ÿ : ç´…é¸[å»Ÿ],æ—¬ç©º[é™·],å’¸æ± [é™·],å¤©å¾·[å»Ÿ]
    
    ğŸ”€ æ™ºèƒ½è·¯ç”±ï¼š
    - æ ‘çŠ¶æ ¼å¼ï¼ˆå«â”‚æˆ–â”œä¸»æ˜Ÿï¼‰â†’ v5.1 ç¨³å®šç‰ˆ
    - ç®€åŒ–æ ¼å¼ï¼ˆä¸å«æ ‘çŠ¶ç¬¦å·ï¼‰â†’ å°è¯• v5.6 Hybrid å¤–æŒ‚ï¼ˆå¤±è´¥æ—¶è‡ªåŠ¨å›é€€ v5.1ï¼‰
    """
    
    # åˆå§‹åŒ–ç»“æœç»“æ„
    result = {
        "basic_info": {"å‘½ä¸»": "", "èº«ä¸»": "", "æ€§åˆ«": "", "çœŸå¤ªé˜³æ—¶": ""},
        "star_map": {},
        "transformations": {"ç”Ÿå¹´å››åŒ–": {}, "æµå¹´å››åŒ–": {}},
        "raw_text": txt,  # ğŸ†• v5.7: ä¿å­˜åŸå§‹æ–‡æœ¬ä¾›åç»­è¡¥ä¸ä½¿ç”¨
        "astro_fingerprint": {}  # ğŸ†• v5.7: åˆå§‹åŒ–å‘½ç†æŒ‡çº¹
    }
    
    # === ğŸ†• v5.6 Hybrid å¤–æŒ‚ï¼šå¯é€‰å¢å¼ºæ¨¡å— ===
    # æ£€æµ‹ç®€åŒ–æ ¼å¼ï¼ˆæ— æ ‘çŠ¶ç¬¦å·ä½†æœ‰å®«ä½+æ˜Ÿæ›œï¼‰
    is_tree_format = "â”‚" in txt or "â”œ" in txt or "â””" in txt
    has_palace_stars = bool(re.search(r"(å‘½å®«|è¿ç§»å®«|è´¢å¸›å®«)[\sï¼š:]+[\u4e00-\u9fa5]", txt))
    use_v56 = False  # æ ‡å¿—ï¼šæ˜¯å¦ä½¿ç”¨ v5.6 è§£æç»“æœ
    
    if not is_tree_format and has_palace_stars:
        try:
            try:
                from .ziwei_txt_hybrid_v56 import parse_wenmo_txt_v56
            except ImportError:
                from ziwei_txt_hybrid_v56 import parse_wenmo_txt_v56
            print("[æ™ºèƒ½è·¯ç”±] ğŸ”€ æ£€æµ‹åˆ°ç®€åŒ–æ ¼å¼ï¼Œå°è¯• v5.6 Hybrid å¤–æŒ‚...")
            v56_result = parse_wenmo_txt_v56(txt)
            
            # éªŒè¯ v5.6 ç»“æœæœ‰æ•ˆæ€§
            if v56_result and v56_result.get("star_map"):
                valid_palaces = [p for p in v56_result["star_map"].values() if p and p.get("ä¸»æ˜Ÿ")]
                if len(valid_palaces) >= 2:  # è‡³å°‘2ä¸ªå®«ä½æœ‰æ•°æ®
                    print(f"[æ™ºèƒ½è·¯ç”±] âœ… v5.6 Hybrid è§£ææˆåŠŸï¼Œæå– {len(valid_palaces)} ä¸ªå®«ä½")
                    use_v56 = True
                    # ä½¿ç”¨ v5.6 çš„è§£æç»“æœ
                    result["star_map"] = v56_result["star_map"]
                    result["transformations"] = v56_result.get("transformations", {"ç”Ÿå¹´å››åŒ–": {}, "æµå¹´å››åŒ–": {}})
                else:
                    print(f"[æ™ºèƒ½è·¯ç”±] âš ï¸ v5.6 æ•°æ®ä¸è¶³ï¼ˆ{len(valid_palaces)} å®«ï¼‰ï¼Œå›é€€ v5.1")
            else:
                print("[æ™ºèƒ½è·¯ç”±] âš ï¸ v5.6 è¿”å›ç©ºæ•°æ®ï¼Œå›é€€ v5.1")
        except Exception as e:
            print(f"[æ™ºèƒ½è·¯ç”±] âš ï¸ v5.6 Hybrid è§£æå¤±è´¥: {e}ï¼Œè‡ªåŠ¨å›é€€ v5.1 ç¨³å®šç‰ˆ")
    else:
        print("[æ™ºèƒ½è·¯ç”±] ğŸ¯ æ£€æµ‹åˆ°æ ‘çŠ¶æ ¼å¼ï¼Œä½¿ç”¨ v5.1 ç¨³å®šè§£æå™¨")

    # ä»…åœ¨æœªä½¿ç”¨ v5.6 æ—¶æ‰§è¡Œ v5.1 æ˜Ÿç›˜è§£æ
    if not use_v56:
        # æå–å‘½ä¸» / èº«ä¸» (æ”¯æŒç¹ç®€ä½“)
        m = re.search(r"å‘½ä¸»[:ï¼š]\s*([^\s\nï¼›;,ï¼Œ]+)", txt)
        s = re.search(r"èº«ä¸»[:ï¼š]\s*([^\s\nï¼›;,ï¼Œ]+)", txt)
        if m: 
            result["basic_info"]["å‘½ä¸»"] = m.group(1).strip()
        if s: 
            result["basic_info"]["èº«ä¸»"] = s.group(1).strip()

        # ç¹ç®€ä½“å®«ä½åç§°æ˜ å°„
        palace_map = {
            "å‘½å®®": "å‘½å®«", "å…„å¼Ÿå®®": "å…„å¼Ÿå®«", "å¤«å¦»å®®": "å¤«å¦»å®«", "å­å¥³å®®": "å­å¥³å®«",
            "è²¡å¸›å®®": "è´¢å¸›å®«", "ç–¾å„å®®": "ç–¾å„å®«", "é·ç§»å®®": "è¿ç§»å®«", "äº¤å‹å®®": "äº¤å‹å®«",
            "åƒ•å½¹å®®": "äº¤å‹å®«",  # ä»†å½¹å®« = äº¤å‹å®«
            "å®˜ç¥¿å®®": "å®˜ç¦„å®«", "ç”°å®…å®®": "ç”°å®…å®«", "ç¦å¾·å®®": "ç¦å¾·å®«", "çˆ¶æ¯å®®": "çˆ¶æ¯å®«"
        }

        # è§£ææ¯ä¸ªå®«ä½
        for palace_trad, palace_simp in palace_map.items():
            # åŒ¹é…å®«ä½å¼€å§‹è¡Œï¼šâ”œè²¡å¸›å®®[æˆŠå­][èº«å®®]
            palace_pattern = rf"[â”œâ””â”‚]\s*{palace_trad}\[([^\]]+)\]"
            palace_match = re.search(palace_pattern, txt, re.MULTILINE)
            
            if palace_match:
                # ğŸ†• v5.7: æå–åœ°æ”¯
                dizhi_raw = palace_match.group(1).strip()
                m_dz = re.search(r"[ç”²ä¹™ä¸™ä¸æˆŠå·±åºšè¾›å£¬ç™¸]?[å­ä¸‘å¯…å¯è¾°å·³åˆæœªç”³é…‰æˆŒäº¥]", dizhi_raw)
                dizhi = m_dz.group(0) if m_dz else ""
                
                # æ‰¾åˆ°å®«ä½èµ·å§‹ä½ç½®
                start_pos = palace_match.end()
                
                # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªå®«ä½çš„ä½ç½®ï¼ˆä½œä¸ºå½“å‰å®«ä½çš„ç»“æŸä½ç½®ï¼‰
                next_palace_pattern = r"[â”œâ””â”‚]\s*(?:å‘½å®®|å…„å¼Ÿå®®|å¤«å¦»å®®|å­å¥³å®®|è²¡å¸›å®®|ç–¾å„å®®|é·ç§»å®®|äº¤å‹å®®|åƒ•å½¹å®®|å®˜ç¥¿å®®|ç”°å®…å®®|ç¦å¾·å®®|çˆ¶æ¯å®®)"
                next_match = re.search(next_palace_pattern, txt[start_pos:], re.MULTILINE)
                end_pos = start_pos + next_match.start() if next_match else len(txt)
                
                # æå–å½“å‰å®«ä½çš„å†…å®¹å—
                palace_block = txt[start_pos:end_pos]
                
                # æå–ä¸»æ˜Ÿã€è¾…æ˜Ÿã€å°æ˜Ÿ
                stars = {"ä¸»æ˜Ÿ": "", "è¾…æ˜Ÿ": "", "å°æ˜Ÿ": "", "åœ°æ”¯": dizhi}  # ğŸ†• v5.7: æ·»åŠ åœ°æ”¯
                
                for star_type in ["ä¸»æ˜Ÿ", "è¼”æ˜Ÿ", "å°æ˜Ÿ"]:
                    # åŒ¹é…ï¼šâ”‚ â”‚ â”œä¸»æ˜Ÿ : ç ´è»[å»Ÿ]
                    star_pattern = rf"[â”œâ”‚]\s*{star_type}\s*[:ï¼š]\s*([^\n]+)"
                    star_match = re.search(star_pattern, palace_block)
                    
                    if star_match:
                        star_text = star_match.group(1).strip()
                        # ğŸ†• v5.7: ä¿ç•™åŸå§‹æ˜Ÿæ›œæ–‡æœ¬ï¼ˆåŒ…å«çŠ¶æ€æ ‡è®°ï¼‰ï¼Œä¾›åç»­è§£æ
                        # æ˜ å°„åˆ°ç®€ä½“é”®å
                        key_map = {"ä¸»æ˜Ÿ": "ä¸»æ˜Ÿ", "è¼”æ˜Ÿ": "è¾…æ˜Ÿ", "å°æ˜Ÿ": "å°æ˜Ÿ"}
                        stars[key_map.get(star_type, star_type)] = star_text
                
                # ä¿å­˜åˆ° star_map (ä½¿ç”¨ç®€ä½“å®«ä½å)
                result["star_map"][palace_simp] = stars
            else:
                # æœªæ‰¾åˆ°è¯¥å®«ä½ï¼Œè¿”å›ç©º dict
                result["star_map"][palace_simp] = {}
        
        print(f"[TXT Patch v2.0] âœ… å·²è§£æ {len([p for p in result['star_map'].values() if p])} ä¸ªå®«ä½ï¼Œå‘½ä¸»={result['basic_info'].get('å‘½ä¸»')}")

    # ## Fallback fix for missing palaces
    try:
        sm = result.get("star_map", {})
        # éœ€è¦å…œåº•ä¿®è¡¥çš„å®«ä½
        need = []
        for k in ("å‘½å®«","äº¤å‹å®«"):
            v = sm.get(k)
            if not isinstance(v, dict) or not (v.get("ä¸»æ˜Ÿ") or v.get("è¾…æ˜Ÿ") or v.get("å°æ˜Ÿ")):
                need.append(k)
        if need:
            # åˆ†åˆ«å¤„ç†æ¯ä¸ªç¼ºå¤±çš„å®«ä½
            for palace in need:
                if palace == "å‘½å®«":
                    keys = ["å‘½\\s*", "å‘½"]
                elif palace == "äº¤å‹å®«":
                    keys = ["äº¤å‹", "åƒ•å½¹", "ä»†å½¹", "å¥´åƒ•", "å¥´ä»†"]
                else:
                    continue
                    
                got = _extract_block(txt, keys)
                if got:
                    sm[palace] = {**{"ä¸»æ˜Ÿ":"","è¾…æ˜Ÿ":"","å°æ˜Ÿ":""}, **got}
            
            result["star_map"] = sm
    except Exception as e:
        print(f"[TXT Patch] Fallback failed: {e}")
        import traceback
        traceback.print_exc()

    # === Ziwei TXT Patch v4.0: å¢å¼ºå››åŒ–/å¤§é™/å°é™/æµå¹´è¯†åˆ« ===
    try:
        extra = _enhance_with_transforms_and_limits(txt, result["star_map"])
        result["transformations"] = {
            "ç”Ÿå¹´å››åŒ–": extra.get("ç”Ÿå¹´å››åŒ–", {}),
            "æµå¹´å››åŒ–": extra.get("æµå¹´å››åŒ–", {})
        }
        result["å¤§é™"] = extra.get("å¤§é™", [])
        result["å°é™"] = extra.get("å°é™", [])
        result["æµå¹´"] = extra.get("æµå¹´", [])
        print(f"[TXT Patch v4.0] âœ… å·²æå–ç”Ÿå¹´å››åŒ–: {extra.get('ç”Ÿå¹´å››åŒ–')}")
    except Exception as e:
        print(f"[TXT Patch v4.0] âš ï¸ å››åŒ–/å¤§é™æ¨¡å—å¼‚å¸¸: {e}")

    # === Ziwei TXT Patch v5.0: æ ¼å±€æ ‡ç­¾ + é£é™©åˆ†æ ===
    try:
        risk_data = _enhance_with_risk_analysis(txt, result["star_map"], result)
        result.update(risk_data)
        print(f"[TXT Patch v5.0] âœ… æ ¼å±€æ ‡ç­¾: {risk_data.get('æ ¼å±€æ ‡ç­¾')}, ç¾éš¾é£é™©: {risk_data.get('ç¾éš¾é¢„æŠ¥', {}).get('é£é™©å€¼')}")
    except Exception as e:
        print(f"[TXT Patch v5.0] âš ï¸ é£é™©åˆ†ææ¨¡å—å¼‚å¸¸: {e}")

    # === Ziwei TXT Patch v5.6+: å››åŒ–æ˜Ÿä¸æ ¼å±€æ ‡ç­¾å¢å¼ºæ¨¡å— ===
    try:
        try:
            from .patch_v5_6p_fourhua import apply_fourhua_patch
            from .patch_v5_6p_pattern import apply_pattern_patch
        except ImportError:
            from patch_v5_6p_fourhua import apply_fourhua_patch
            from patch_v5_6p_pattern import apply_pattern_patch
        
        # åº”ç”¨å››åŒ–å¢å¼ºè¡¥ä¸ï¼ˆä½¿ç”¨åŸå§‹æ–‡æœ¬ï¼‰
        result = apply_fourhua_patch(result, txt)
        # åº”ç”¨æ ¼å±€æ ‡ç­¾å¢å¼ºè¡¥ä¸
        result = apply_pattern_patch(result)
        print(f"[TXT Patch v5.6+] âœ… å·²æ³¨å…¥å¢å¼ºå››åŒ–æ˜Ÿä¸æ ¼å±€æ ‡ç­¾")
    except Exception as e:
        print(f"[TXT Patch v5.6+] âš ï¸ å¢å¼ºè¡¥ä¸è°ƒç”¨å¤±è´¥: {e}")

    # === Ziwei TXT Patch v5.7: æ‰©å±•ä¿®è®¢ç‰ˆï¼ˆåœ°æ”¯+çŠ¶æ€+å››åŒ–å¢å¼ºï¼‰ ===
    try:
        try:
            from .patch_v5_7_palace_dz import extract_palace_dz
            from .patch_v5_7_starstate import extract_starstate
            from .patch_v5_7_fourhua_auto import extract_fourhua
            from .patch_v5_7_normalizer import normalize_result
        except ImportError:
            from patch_v5_7_palace_dz import extract_palace_dz
            from patch_v5_7_starstate import extract_starstate
            from patch_v5_7_fourhua_auto import extract_fourhua
            from patch_v5_7_normalizer import normalize_result
        
        # åº”ç”¨ v5.7 è¡¥ä¸é“¾
        result = extract_palace_dz(result)      # åœ°æ”¯æå–
        result = extract_starstate(result)      # æ˜Ÿæ›œçŠ¶æ€è§£æ
        result = extract_fourhua(result)        # å››åŒ–æ˜Ÿè‡ªåŠ¨æå–ï¼ˆå¢å¼ºç‰ˆï¼‰
        result = normalize_result(result)       # æ•°æ®æ ‡å‡†åŒ–
        print(f"[TXT Patch v5.7] âœ… å·²åº”ç”¨æ‰©å±•ä¿®è®¢ç‰ˆï¼ˆåœ°æ”¯+çŠ¶æ€+å››åŒ–å¢å¼ºï¼‰")
    except Exception as e:
        print(f"[TXT Patch v5.7] âš ï¸ æ‰©å±•è¡¥ä¸è°ƒç”¨å¤±è´¥: {e}")

    # === Ziwei TXT Patch v5.8: è¡Œå†…å››åŒ–æŠ“å– + æ ¼å±€é£é™©å¢å¼º ===
    try:
        try:
            from .patch_v5_8_fourhua_auto import patch_transformations
            from .patch_v5_8_patterns import patch_patterns_and_risk
        except ImportError:
            from patch_v5_8_fourhua_auto import patch_transformations
            from patch_v5_8_patterns import patch_patterns_and_risk
        
        # åº”ç”¨ v5.8 è¡¥ä¸é“¾
        result = patch_transformations(result)      # è¡Œå†…å››åŒ– + ï¼ˆå¯é€‰ï¼‰å¤©å¹²å››åŒ–
        result = patch_patterns_and_risk(result)    # æ ¼å±€æ ‡ç­¾ + è¿ç§»å®«é£é™© Ã—2
        print(f"[TXT Patch v5.8] âœ… å·²åº”ç”¨å››åŒ–æŠ“å–ä¸æ ¼å±€é£é™©å¢å¼º")
    except Exception as e:
        print(f"[TXT Patch v5.8] âš ï¸ v5.8 è¡¥ä¸è°ƒç”¨å¤±è´¥: {e}")

    # === Ziwei TXT Patch v5.9.3: å››åŒ–å¤©å¹²æ¨ç®—ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰ ===
    try:
        try:
            from .patch_v5_9_3_tiangan_fourhua import patch_tiangan_fourhua
        except ImportError:
            from patch_v5_9_3_tiangan_fourhua import patch_tiangan_fourhua
        
        # è‹¥å››åŒ–æ•°æ®ä¸ºç©ºï¼Œæ ¹æ®ç”Ÿå¹´å¤©å¹²è‡ªåŠ¨æ¨ç®—
        result = patch_tiangan_fourhua(result)
    except Exception as e:
        print(f"[TXT Patch v5.9.3] âš ï¸ å¤©å¹²æ¨ç®—è¡¥ä¸è°ƒç”¨å¤±è´¥: {e}")

    # === Ziwei TXT Patch v5.9: å››åŒ–æ ‡ç­¾è‡ªåŠ¨å†…åµŒæ˜Ÿæ›œ ===
    try:
        try:
            from .patch_v5_9_fourhua_embed import patch_fourhua_embed
        except ImportError:
            from patch_v5_9_fourhua_embed import patch_fourhua_embed
        
        # ä» transformations åæ¨å››åŒ–åˆ°æ˜Ÿæ›œæ ‡ç­¾
        result = patch_fourhua_embed(result)
    except Exception as e:
        print(f"[TXT Patch v5.9] âš ï¸ å››åŒ–å†…åµŒè¡¥ä¸è°ƒç”¨å¤±è´¥: {e}")

    return result


def parse_wenmo_auto(obj_or_txt):
    """
    è‡ªåŠ¨åˆ¤æ–­è¾“å…¥æ˜¯ JSON è¿˜æ˜¯ TXTï¼Œå¹¶è°ƒç”¨ç›¸åº”è§£æå™¨ã€‚
    """
    if isinstance(obj_or_txt, str):
        # å¯èƒ½æ˜¯ JSON å­—ç¬¦ä¸²æˆ– TXT æ–‡æœ¬
        try:
            j = json.loads(obj_or_txt)
            print("[TXT Patch] ğŸ“„ è¾“å…¥ä¸º JSON å­—ç¬¦ä¸²ï¼Œè½¬ä¸º dict æ ¼å¼")
            return j
        except Exception:
            print("[TXT Patch] ğŸ“ æ£€æµ‹åˆ° TXT å†…å®¹ï¼Œå¯ç”¨ TXTâ†’JSON è½¬æ¢æ¨¡å—")
            return parse_wenmo_txt_to_json(obj_or_txt)
    elif isinstance(obj_or_txt, dict):
        return obj_or_txt
    else:
        raise TypeError(f"Unsupported type: {type(obj_or_txt)}")


def _normalize_palace_name(raw: str) -> str:
    name = (raw or "").strip()
    name = name.replace("å®®", "å®«")
    name = re.sub(r"\s+", "", name)   # å‘½  å®« -> å‘½å®«
    # åŒä¹‰è¯å½’å¹¶
    alias = {
        "äº¤å‹å®«": {"äº¤å‹å®«","åƒ•å½¹å®®","ä»†å½¹å®«","å¥´åƒ•å®®","å¥´ä»†å®«"},
    }
    for k, v in alias.items():
        if name in v:
            return k
    return name


def _extract_block(text, palace_keys):
    # é’ˆå¯¹å‘½å®«/äº¤å‹å®«ä¸“ç”¨ï¼šå…è®¸æ ‡é¢˜ç©ºæ ¼ã€ç¬¬äºŒä¸ª[]ã€ç¹ç®€æ··æ’
    for pk in palace_keys:
        # åŒ¹é…å®«ä½æ ‡é¢˜è¡Œ + åç»­æ‰€æœ‰ä»¥â”‚å¼€å¤´çš„å†…å®¹è¡Œï¼ˆç›´åˆ°é‡åˆ°éâ”‚è¡Œæˆ–æ–‡ä»¶ç»“æŸï¼‰
        pat = rf"""(?P<hdr>^[\sâ”‚â”œâ””]*[â”‚â”œâ””]?\s*{pk}(?:\s*)[å®«å®®]\[[^\]]+\](?:\[[^\]]+\])?\s*$)
(?P<body>(?:^[â”‚â”œâ””\s]+.*$\n?)+)
"""
        m = re.search(pat, text, re.M)
        if not m:
            continue
        body = m.group("body")
        def pick(label):
            mm = re.search(rf"^[â”‚â”œâ””â”¤\s]*.*?{label}\s*[:ï¼š]\s*([^\n\r]+)", body, re.M)
            if not mm:
                return ""
            raw = mm.group(1).strip()
            # ğŸ†• v5.7: ä¿ç•™åŸå§‹æ˜Ÿæ›œæ–‡æœ¬ï¼ˆåŒ…å«çŠ¶æ€æ ‡è®°ï¼‰
            return raw
        # å…¼å®¹ è¼”æ˜Ÿ/è¾…æ˜Ÿ
        fu = pick("è¾…æ˜Ÿ") or pick("è¼”æ˜Ÿ")
        return {
            "ä¸»æ˜Ÿ": pick("ä¸»æ˜Ÿ"),
            "è¾…æ˜Ÿ": fu,
            "å°æ˜Ÿ": pick("å°æ˜Ÿ"),
        }
    return None



# =======================================================
# Ziwei TXT Patch v4.0 â€” å››åŒ– + å¤§é™ + å°é™ + æµå¹´è¯†åˆ«
# =======================================================

def _parse_four_transforms(text):
    res = {"ç¦„": "", "æƒ": "", "ç§‘": "", "å¿Œ": ""}
    m = re.search(r"ç”Ÿå¹´å››åŒ–[:ï¼š]?\s*ç¦„.?([^\sã€]+).*æƒ.?([^\sã€]+).*ç§‘.?([^\sã€]+).*å¿Œ.?([^\sã€]+)", text)
    if m:
        res.update({"ç¦„": m.group(1), "æƒ": m.group(2), "ç§‘": m.group(3), "å¿Œ": m.group(4)})
    return res

def _parse_flow_transforms(text):
    res = {"ç¦„": "", "æƒ": "", "ç§‘": "", "å¿Œ": ""}
    m = re.search(r"æµå¹´å››åŒ–[:ï¼š]?\s*ç¦„.?([^\sã€]+).*æƒ.?([^\sã€]+).*ç§‘.?([^\sã€]+).*å¿Œ.?([^\sã€]+)", text)
    if m:
        res.update({"ç¦„": m.group(1), "æƒ": m.group(2), "ç§‘": m.group(3), "å¿Œ": m.group(4)})
    return res

def _parse_decades(text):
    limits = []
    for seg in re.findall(r"å¤§é™[:ï¼š]?\s*(\d{2}-\d{2})\s*å²\s*([ç”²ä¹™ä¸™ä¸æˆŠå·±åºšè¾›å£¬ç™¸]\S+)\s*â†’\s*([^\n\r]+)", text):
        limits.append({"åŒºé—´": seg[0], "åœ°æ”¯": seg[1], "æ˜Ÿæ›œ": seg[2].strip()})
    return limits

def _parse_minor(text):
    limits = []
    for seg in re.findall(r"å°é™[:ï¼š]?\s*(\d{1,2})å²\s*([^\s]+)\s*([^\n\r]+)", text):
        limits.append({"å¹´é¾„": seg[0], "å®«ä½": seg[1], "æ˜Ÿæ›œ": seg[2].strip()})
    return limits

def _parse_yearflow(text):
    limits = []
    for seg in re.findall(r"æµå¹´\s*(\d{4})[:ï¼š]?\s*([^\n\r]+)", text):
        limits.append({"å¹´ä»½": seg[0], "å†…å®¹": seg[1].strip()})
    return limits

# === Hook æ³¨å…¥åˆ°ä¸»å‡½æ•°ç»“å°¾ ===
def _enhance_with_transforms_and_limits(raw_text, star_map):
    return {
        "ç”Ÿå¹´å››åŒ–": _parse_four_transforms(raw_text),
        "æµå¹´å››åŒ–": _parse_flow_transforms(raw_text),
        "å¤§é™": _parse_decades(raw_text),
        "å°é™": _parse_minor(raw_text),
        "æµå¹´": _parse_yearflow(raw_text)
    }



# =======================================================
# Ziwei TXT Patch v5.0 â€” æ ¼å±€ + ç¾éš¾é¢„æŠ¥ + é£é™©å› å­
# =======================================================

def _detect_patterns(star_map):
    tags = []
    s = json.dumps(star_map, ensure_ascii=False)
    if re.search("ç´«å¾®.*å¤©åºœ", s): tags.append("ç´«åºœæœå£")
    if re.search("ä¸ƒæ®º.*ç ´è».*è²ªç‹¼", s): tags.append("æ®ºç ´ç‹¼")
    if re.search("å¤ªé™½.*å¤ªé™°", s): tags.append("æ—¥æœˆå¹¶æ˜")
    if re.search("æ­¦æ›².*å¤©ç›¸", s): tags.append("è²¡å®˜é›™ç¾")
    if re.search("å»‰è².*è²ªç‹¼", s): tags.append("æ¡ƒèŠ±é‡")
    if re.search("å¤©æ¢.*å¤©åŒ", s): tags.append("æ…ˆå–„æ ¼")
    return tags

def _calc_disaster_risk(star_map):
    danger_keywords = ['åŒ–å¿Œ','ç¾Š','é™€','é“ƒ','ç«','ç©º','åŠ«','åˆ‘','ç…']
    risk_score = 0
    for palace, data in star_map.items():
        content = json.dumps(data, ensure_ascii=False)
        matches = sum(1 for k in danger_keywords if k in content)
        if matches: risk_score += matches * 0.1
    return min(round(risk_score,2),1.0)

def _calc_flow_risk(flow_data, star_map):
    if not flow_data: return 0.3
    risk = 0
    if any("å¿Œ" in v for v in flow_data.values() if v): risk += 0.2
    if any("æ¬Š" in v for v in flow_data.values() if v): risk += 0.1
    if "ç–¾å„" in json.dumps(star_map,ensure_ascii=False): risk += 0.1
    return min(round(0.3 + risk,2),1.0)

def _calc_monthly_risk(text):
    month_risks = {}
    for seg in re.findall(r"æµæœˆ(\d{1,2})[:ï¼š]?\s*([^\n\r]+)", text):
        m, content = seg
        risk = 0.3
        if "å¿Œ" in content: risk += 0.2
        if "ç…" in content: risk += 0.1
        if "å–œ" in content or "ç§‘" in content: risk -= 0.1
        month_risks[int(m)] = round(max(min(risk,1.0),0.1),2)
    return month_risks

def _enhance_with_risk_analysis(raw_text, star_map, result):
    patterns = _detect_patterns(star_map)
    # âœ… v5.1: ä½¿ç”¨å¢å¼ºç‰ˆè¿ç§»å®«é£é™©åˆ†æ
    disaster_data = _calc_disaster_risk_v51(star_map)
    flow_risk = _calc_flow_risk(result.get("æµå¹´å››åŒ–",{}), star_map)
    monthly = _calc_monthly_risk(raw_text)
    
    risk_result = {
        "æ ¼å±€æ ‡ç­¾": patterns,
        "ç¾éš¾é¢„æŠ¥": disaster_data,
        "æµå¹´é£é™©å› å­": flow_risk,
        "æµæœˆé£é™©å› å­": monthly
    }
    
    # å¦‚æœå­˜åœ¨è¿ç§»å®«é¢„è­¦ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
    if disaster_data.get("è¿ç§»å®«é¢„è­¦è¯¦æƒ…"):
        risk_result["è¿ç§»å®«é¢„è­¦è¯¦æƒ…"] = disaster_data["è¿ç§»å®«é¢„è­¦è¯¦æƒ…"]
    
    return risk_result



# =======================================================
# Ziwei TXT Patch v5.1 â€” è¿ç§»å®«é‡ç‚¹ç¾è±¡åˆ†æå¼ºåŒ–
# =======================================================

def _calc_disaster_risk_v51(star_map):
    danger_keywords = ['åŒ–å¿Œ','ç¾Š','é™€','é“ƒ','ç«','ç©º','åŠ«','åˆ‘','ç…']
    risk_score = 0
    migration_detail = {"å®«ä½": "è¿ç§»å®«", "å‡¶è±¡": [], "å¤‡æ³¨": ""}

    for palace, data in star_map.items():
        content = json.dumps(data, ensure_ascii=False)
        matches = [k for k in danger_keywords if k in content]
        if not matches:
            continue
        add_score = len(matches) * 0.1
        if palace == "è¿ç§»å®«":
            add_score *= 2  # æƒé‡åŠ å€
            migration_detail["å‡¶è±¡"].extend(matches)
            if any("åŒ–å¿Œ" in k for k in matches):
                add_score += 0.3
                migration_detail["å¤‡æ³¨"] = "è¿ç§»å®«åŒ–å¿Œâ€”â€”é‡å¤§å¤–åŠ¨å‡¶è±¡"
            elif any(k in content for k in ["ä¸ƒæ®º","ç ´è»","è²ªç‹¼"]):
                add_score += 0.2
                migration_detail["å¤‡æ³¨"] = "è¿ç§»å®«åŠ¨ç…å è±¡"
        risk_score += add_score

    migration_detail["å‡¶è±¡"] = list(set(migration_detail["å‡¶è±¡"]))
    return {
        "é£é™©å€¼": min(round(risk_score,2),1.0),
        "è¿ç§»å®«é¢„è­¦è¯¦æƒ…": migration_detail if migration_detail["å‡¶è±¡"] else None,
        "è¯´æ˜": f"ç´¯è®¡å±é™©æ˜Ÿæ›œå‡ºç° {int(risk_score*10)} æ¬¡"
    }


# ===========================================
# Ziwei AI Patch v5.3 - Parser å±‚å¢å¼º
# ===========================================
def _extract_four_transforms(lines):
    result = {"ç”Ÿå¹´å››åŒ–": {}, "æµå¹´å››åŒ–": {}}
    for line in lines:
        if "ç”Ÿå¹´å››åŒ–" in line:
            parts = re.findall(r"(ç¦„|æƒ|ç§‘|å¿Œ).*?[â†’â†’>â†’:ï¼š ]+([^\sã€]+)", line)
            for k, v in parts:
                result["ç”Ÿå¹´å››åŒ–"][k] = v
        elif "æµå¹´å››åŒ–" in line:
            parts = re.findall(r"(ç¦„|æƒ|ç§‘|å¿Œ).*?[â†’â†’>â†’:ï¼š ]+([^\sã€]+)", line)
            for k, v in parts:
                result["æµå¹´å››åŒ–"][k] = v
    return result

def _extract_limits(lines):
    result = {"å¤§é™": "", "å°é™": "", "æµå¹´": ""}
    for line in lines:
        if "å¤§é™" in line: result["å¤§é™"] = line.strip()
        if "å°é™" in line: result["å°é™"] = line.strip()
        if "æµå¹´" in line: result["æµå¹´"] = line.strip()
    return result
