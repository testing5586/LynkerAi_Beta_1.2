# ================================
# Phase 3.2 + 3.3ï¼šRAG Chat + å‘é‡æ£€ç´¢ï¼ˆä¸­æ–‡å…è´¹æ–¹æ¡ˆï¼‰
# ================================

echo "ğŸ“¦ å®‰è£…ä¾èµ–ï¼ˆå¦‚å·²å®‰è£…ä¼šè‡ªåŠ¨è·³è¿‡ï¼‰..."
pip -q install faiss-cpu sentence-transformers jieba pdfminer.six python-docx > /dev/null 2>&1 || true

# ---------- 1) å‘é‡ç´¢å¼•å™¨ï¼šæ‰«æ Vaultã€åˆ†å—ã€åµŒå…¥ã€ä¿å­˜ ----------
cat > vector_indexer.py << 'PY'
import os, json, re, time, argparse
from pathlib import Path
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
from pdfminer.high_level import extract_text as pdf_extract
from docx import Document

VAULT_DIR = Path("lynker_master_vault")
VSTORE_DIR = VAULT_DIR / "vector_store"
VSTORE_DIR.mkdir(parents=True, exist_ok=True)
INDEX_FILE = VSTORE_DIR / "faiss.index"
META_FILE  = VSTORE_DIR / "meta.json"

# ç¼“å­˜æ¨¡å‹ï¼ˆä¸é¡¹ç›®å…¶å®ƒå¤„ä¸€è‡´çš„å…è´¹ä¸­æ–‡å‘é‡æ¨¡å‹ï¼‰
_model = None
def get_model():
    global _model
    if _model is None:
        print("ğŸ§  Loading embedding model (cached if same process)...")
        _model = SentenceTransformer("shibing624/text2vec-base-chinese")
    return _model

def read_text(path: Path) -> str:
    p = str(path).lower()
    try:
        if p.endswith(".md") or p.endswith(".txt"):
            return path.read_text(encoding="utf-8", errors="ignore")
        if p.endswith(".pdf"):
            return pdf_extract(str(path))
        if p.endswith(".docx"):
            doc = Document(str(path))
            return "\n".join([p.text for p in doc.paragraphs])
    except Exception as e:
        print(f"âš ï¸ è¯»å–å¤±è´¥ {path.name}: {e}")
    return ""

def split_chunks(text, chunk_size=600, overlap=120):
    text = re.sub(r"\s+", " ", text).strip()
    if not text: return []
    chunks = []
    start = 0
    while start < len(text):
        end = min(len(text), start + chunk_size)
        chunks.append(text[start:end])
        start = end - overlap
        if start < 0:
            start = 0
        if end == len(text): break
    return chunks

def scan_docs():
    exts = (".md", ".txt", ".pdf", ".docx")
    for cat in ["project_docs", "dev_brainstorm", "api_docs", "memory"]:
        folder = VAULT_DIR / cat
        if not folder.exists(): continue
        for f in folder.glob("*"):
            if f.is_file() and f.suffix.lower() in exts:
                yield cat, f

def build_or_update(rebuild=False):
    meta = {"items": []}
    xb = None
    index = None

    if (INDEX_FILE.exists() and META_FILE.exists() and not rebuild):
        # å¢é‡è½½å…¥
        print("ğŸ” Loading existing index for incremental update...")
        index = faiss.read_index(str(INDEX_FILE))
        meta = json.loads(META_FILE.read_text(encoding="utf-8"))

    model = get_model()
    added = 0
    for cat, f in scan_docs():
        # è·³è¿‡å·²ç´¢å¼•çš„æ–‡ä»¶ï¼ˆç®€æ˜“åˆ¤æ–­ï¼šçœ‹æ˜¯å¦å·²æœ‰ç›¸åŒ file_idï¼‰
        file_id = f"{cat}/{f.name}"
        if not rebuild and any(it["file_id"] == file_id for it in meta["items"]):
            continue

        txt = read_text(f)
        chunks = split_chunks(txt)
        if not chunks: 
            continue
        embs = model.encode(chunks, normalize_embeddings=True)
        embs = np.array(embs, dtype="float32")

        if index is None:
            index = faiss.IndexFlatIP(embs.shape[1])  # ä½™å¼¦ç›¸ä¼¼åº¦ç”¨å†…ç§¯ï¼ˆå·²normalizeï¼‰
        index.add(embs)

        # è®°å½• metadata
        for i, c in enumerate(chunks):
            meta["items"].append({
                "file_id": file_id,
                "category": cat,
                "chunk_id": i,
                "text": c
            })
        added += len(chunks)
        print(f"ğŸ“š Indexed: {file_id} ({len(chunks)} chunks)")

    if index is None:
        print("âš ï¸ æ²¡æœ‰æ–‡æ¡£å¯ç´¢å¼•ã€‚")
        return

    faiss.write_index(index, str(INDEX_FILE))
    META_FILE.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"âœ… ç´¢å¼•å®Œæˆï¼š{len(meta['items'])} chunks | æ–°å¢ {added} chunks")

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--rebuild", action="store_true", help="é‡å»ºç´¢å¼•ï¼ˆå¿½ç•¥å†å²ï¼‰")
    args = ap.parse_args()
    t0 = time.time()
    build_or_update(rebuild=args.rebuild)
    print(f"â±ï¸ ç”¨æ—¶ï¼š{time.time()-t0:.2f}s")
PY
echo "âœ… vector_indexer.py å·²åˆ›å»º"

# ---------- 2) å‘é‡æ£€ç´¢å™¨ï¼šåŠ è½½ç´¢å¼•ã€TopK æœç´¢ ----------
cat > vector_search.py << 'PY'
import json, numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer
import faiss

VAULT_DIR = Path("lynker_master_vault")
VSTORE_DIR = VAULT_DIR / "vector_store"
INDEX_FILE = VSTORE_DIR / "faiss.index"
META_FILE  = VSTORE_DIR / "meta.json"

_model = None
_index = None
_meta  = None

def get_model():
    global _model
    if _model is None:
        _model = SentenceTransformer("shibing624/text2vec-base-chinese")
    return _model

def load_store():
    global _index, _meta
    if _index is None:
        _index = faiss.read_index(str(INDEX_FILE))
    if _meta is None:
        _meta = json.loads(META_FILE.read_text(encoding="utf-8"))
    return _index, _meta

def search(query: str, topk=5):
    model = get_model()
    index, meta = load_store()
    q = model.encode([query], normalize_embeddings=True).astype("float32")
    D, I = index.search(q, topk)
    hits = []
    for score, idx in zip(D[0], I[0]):
        if idx < 0 or idx >= len(meta["items"]): continue
        item = meta["items"][idx]
        hits.append({
            "score": float(score),
            **item
        })
    return hits

if __name__ == "__main__":
    from pprint import pprint
    if not INDEX_FILE.exists():
        print("âš ï¸ ç´¢å¼•ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œï¼špython vector_indexer.py --rebuild")
    else:
        pprint(search("åŒå‘½åŒ¹é… ä¸ å…«å­— éªŒè¯"))
PY
echo "âœ… vector_search.py å·²åˆ›å»º"

# ---------- 3) Chat é¡µé¢ï¼ˆçº¯å‰ç«¯ï¼‰ ----------
cat > static_chat.html << 'HTML'
<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<title>Lynker Master AI â€” Chat</title>
<style>
  body{font-family:Inter,system-ui,-apple-system; background:#0f111a; color:#e5e5e5; margin:0;}
  .wrap{max-width:900px;margin:0 auto;padding:24px;}
  .header{display:flex;gap:12px;align-items:center;justify-content:space-between}
  .btn{background:#6B46C1;border:0;color:#fff;padding:10px 14px;border-radius:10px;cursor:pointer}
  .pill{background:#6936f31a;border:1px solid #6B46C1;color:#cbb6ff;padding:6px 10px;border-radius:999px}
  .box{background:#151826;border:1px solid #2a2e45;border-radius:16px;padding:16px;margin-top:16px}
  .msg{padding:12px 14px;border-radius:12px;margin:8px 0}
  .user{background:#1f2340}
  .ai{background:#161a2e}
  .sources{font-size:12px;color:#a8b3cf}
</style>
</head>
<body>
  <div class="wrap">
    <div class="header">
      <h2>ğŸ’¬ Lynker Master AI</h2>
      <div>
        <a class="pill" href="/api/master-ai/context" target="_blank">ğŸ“š Vault</a>
        <a class="pill" href="/api/master-ai/upload-stats" target="_blank">ğŸ“Š Stats</a>
        <a class="btn" href="/upload">ğŸ“¤ ä¸Šä¼ åˆ° Vault</a>
      </div>
    </div>

    <div id="chat" class="box"></div>

    <div class="box">
      <input id="q" placeholder="ä¾‹å¦‚ï¼šè¯·æ ¹æ®Vaultè¯´æ˜ï¼Œå¸®æˆ‘æè¿°â€˜çœŸå‘½ç›˜éªŒè¯â€™çš„æ ¸å¿ƒæµç¨‹" style="width:100%;padding:12px;border-radius:10px;border:1px solid #2a2e45;background:#0f111a;color:#e5e5e5"/>
      <button class="btn" style="margin-top:12px" onclick="ask()">å‘é€</button>
    </div>
  </div>

<script>
const chat = document.getElementById('chat');
function push(role, text){
  const d = document.createElement('div');
  d.className = 'msg ' + (role==='user'?'user':'ai');
  d.innerHTML = text;
  chat.appendChild(d);
  chat.scrollTop = chat.scrollHeight;
}
async function ask(){
  const q = document.getElementById('q').value.trim();
  if(!q) return;
  push('user', 'ğŸ§‘â€ğŸ’» ' + q);
  document.getElementById('q').value = '';
  const res = await fetch('/api/master-ai/chat', {
    method:'POST', headers:{'Content-Type':'application/json'},
    body: JSON.stringify({query:q, topk:5})
  });
  const data = await res.json();
  if(data.status==='ok'){
    let s = 'ğŸ¤– ' + data.answer;
    if(data.citations && data.citations.length){
      s += '<div class="sources">å‚è€ƒç‰‡æ®µï¼š<ul>'
      for(const c of data.citations){
        s += '<li>['+c.category+'] ' + c.file_id + ' #'+c.chunk_id+'ï¼ˆç›¸å…³åº¦'+c.score.toFixed(3)+'ï¼‰</li>';
      }
      s += '</ul></div>';
    }
    push('ai', s);
  }else{
    push('ai', 'âš ï¸ ' + data.message);
  }
}
</script>
</body>
</html>
HTML
echo "âœ… Chat å‰ç«¯ static_chat.html å·²åˆ›å»º"

# ---------- 4) å°† Chat API & RAG é›†æˆåˆ°ç°æœ‰ master_ai_uploader_api.py ----------
python - << 'PY'
import io, re, sys, os, json
p="master_ai_uploader_api.py"
src=open(p,"r",encoding="utf-8").read()

# è‹¥å·²é›†æˆåˆ™è·³è¿‡
if "/api/master-ai/chat" in src and "static_chat.html" in src:
    print("â„¹ï¸ master_ai_uploader_api.py å·²åŒ…å« Chat è·¯ç”±ï¼Œè·³è¿‡ä¿®æ”¹ã€‚")
    sys.exit(0)

injection = r'''
# === RAG Chat ä¾èµ– ===
from flask import send_file, jsonify
from vector_search import search as rag_search
from vector_indexer import build_or_update as rag_reindex

@app.route("/chat")
def chat_page():
    # ç®€å•åœ°è¿”å›é™æ€HTMLï¼ˆå·²å†…åµŒè„šæœ¬ä¸æ ·å¼ï¼‰
    return send_file("static_chat.html")

@app.route("/api/master-ai/chat", methods=["POST"])
def master_ai_chat():
    """RAGï¼šä» Vault ä¸­æ£€ç´¢ç›¸å…³ç‰‡æ®µå¹¶ç”Ÿæˆç®€è¦å›ç­”ï¼ˆæ— å¤–éƒ¨LLMï¼Œæ‘˜è¦æ¨¡æ¿å¼ï¼‰"""
    try:
        data = request.get_json(force=True)
        query = (data.get("query") or "").strip()
        topk  = int(data.get("topk") or 5)
        if not query:
            return jsonify({"status":"error","message":"ç¼ºå°‘ query"}), 400

        # æ£€ç´¢
        hits = rag_search(query, topk=topk)
        if not hits:
            return jsonify({"status":"ok","answer":"æ²¡æœ‰åœ¨ Vault ä¸­æ‰¾åˆ°ç›¸å…³èµ„æ–™ã€‚", "citations":[]})

        # ç»„è£…ä¸€ä¸ªæ¨¡æ¿å¼çš„â€œå›ç­”â€
        bullets = []
        for h in hits:
            txt = h["text"].strip()
            if len(txt)>180: txt = txt[:180]+"..."
            bullets.append(f"â€¢ æ¥è‡ªã€Š{h['file_id']}ã€‹ï¼š{txt}")
        answer = "åŸºäºçŸ¥è¯†åº“æ£€ç´¢ï¼Œæˆ‘æ‰¾åˆ°ä»¥ä¸‹è¦ç‚¹ï¼š\n" + "\n".join(bullets) + "\n\nï¼ˆä»¥ä¸Šä¸ºè‡ªåŠ¨æ£€ç´¢æ‘˜è¦ï¼Œè¯¦æƒ…è¯·æŸ¥çœ‹å¼•ç”¨ç‰‡æ®µä¸åŸæ–‡æ¡£ï¼‰"

        return jsonify({"status":"ok","answer":answer,"citations":hits})
    except Exception as e:
        return jsonify({"status":"error","message":str(e)}), 500
'''

# åœ¨æ–‡ä»¶æœ«å°¾å‰æ’å…¥ï¼ˆä¿ç•™åŸæ¥å£ï¼‰
src = src + "\n" + injection

open(p,"w",encoding="utf-8").write(src)
print("âœ… å·²å‘ master_ai_uploader_api.py æ³¨å…¥ Chat è·¯ç”±ä¸ RAG API")
PY

# ---------- 5) åœ¨ä¸Šä¼ æˆåŠŸåè‡ªåŠ¨è§¦å‘â€œå¢é‡å‘é‡æ›´æ–°â€ ----------
python - << 'PY'
p="master_ai_uploader_api.py"
s=open(p,"r",encoding="utf-8").read()
anchor="result = subprocess.getoutput(f\"python master_ai_importer.py {filepath}\")"
if anchor in s and "rag_reindex()" not in s:
    s=s.replace(anchor, anchor + "\n\n    # å‘é‡åº“å¢é‡æ›´æ–°ï¼ˆä»…ç´¢å¼•æ–°æ–‡ä»¶ï¼‰\n    try:\n        rag_reindex(rebuild=False)\n    except Exception as _e:\n        print(f\"âš ï¸ å‘é‡åº“å¢é‡æ›´æ–°å¤±è´¥ï¼š{_e}\")\n")
    open(p,"w",encoding="utf-8").write(s)
    print("âœ… ä¸Šä¼ åå·²æ¥å…¥å‘é‡åº“å¢é‡æ›´æ–°")
else:
    print("â„¹ï¸ å·²å­˜åœ¨å¢é‡æ›´æ–°æˆ–æœªæ‰¾åˆ°æ³¨å…¥ç‚¹ï¼Œè·³è¿‡ã€‚")
PY

# ---------- 6) é¦–æ¬¡å…¨é‡æ„å»ºå‘é‡ç´¢å¼• ----------
echo "ğŸ”§ æ­£åœ¨å…¨é‡æ„å»ºå‘é‡ç´¢å¼•ï¼ˆå¦‚æ— æ–‡æ¡£ä¼šæç¤º0ï¼‰..."
python vector_indexer.py --rebuild

# ---------- 7) å¯åŠ¨ä¸»æœåŠ¡ï¼ˆç«¯å£æ²¿ç”¨ä½ å½“å‰çš„ 8008ï¼‰ ----------
echo "ğŸš€ å¯åŠ¨ä¸»æœåŠ¡ï¼š/uploadã€/api/master-ai/*ã€/chat å…¨éƒ¨åœ¨åŒä¸€åº”ç”¨ä¸­"
python master_ai_uploader_api.py
