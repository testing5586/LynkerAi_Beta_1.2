# ==========================================================
# LynkerAI TrueChart Verifier v3.3
# AIè®°å¿†æ•°æ®åº“ç‰ˆï¼ˆSupabase å†™å…¥æœºåˆ¶ï¼‰
# æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡å‘é‡åŒ– + å¹¶å‘éªŒè¯
# ==========================================================

import json, os, re, time
import numpy as np
from datetime import datetime
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
from sentence_transformers import SentenceTransformer, util
from supabase_init import init_supabase

# ==================================================
# æ¨¡å‹ç¼“å­˜æœºåˆ¶ï¼ˆæ–¹æ¡ˆ Aï¼‰
# ==================================================
@lru_cache(maxsize=1)
def get_semantic_model():
    """åŠ è½½å¹¶ç¼“å­˜ä¸­æ–‡è¯­ä¹‰æ¨¡å‹ï¼ŒåªåŠ è½½ä¸€æ¬¡"""
    print("ğŸ§  Loading high-semantic Chinese model (cached)...")
    return SentenceTransformer("shibing624/text2vec-base-chinese")

# ä½¿ç”¨ç¼“å­˜æ¨¡å‹ï¼ˆç¬¬ä¸€æ¬¡åŠ è½½åå°†å¸¸é©»å†…å­˜ï¼‰
model = get_semantic_model()
print("âœ… Model loaded successfully!")

# ----------------------------------------------------------
# åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯
# ----------------------------------------------------------
supabase = init_supabase()

# ----------------------------------------------------------
# è¾…åŠ©å‡½æ•°
# ----------------------------------------------------------
def semantic_similarity(a: str, b: str) -> float:
    emb1 = model.encode(a, convert_to_tensor=True)
    emb2 = model.encode(b, convert_to_tensor=True)
    return round(float(util.pytorch_cos_sim(emb1, emb2).item()), 4)

def batch_similarity(model_instance, events: list, chart_features: list):
    """
    æ‰¹é‡è®¡ç®—æ¯ä¸ªäº‹ä»¶ä¸å‘½ç›˜ç‰¹å¾çš„æœ€ä½³ç›¸ä¼¼åº¦
    å‚æ•°ï¼š
        model_instance: è¯­ä¹‰æ¨¡å‹å®ä¾‹
        events: äººç”Ÿäº‹ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªäº‹ä»¶åŒ…å« desc å’Œ weight
        chart_features: å‘½ç›˜ç‰¹å¾åˆ—è¡¨
    è¿”å›ï¼š
        æ›´æ–°äº† similarity å’Œ top_match_feature çš„äº‹ä»¶åˆ—è¡¨
    """
    if not events or not chart_features:
        return events
    
    # æå–äº‹ä»¶æè¿°å’Œå‘½ç›˜ç‰¹å¾æ–‡æœ¬
    event_texts = [e.get("desc", "") for e in events]
    
    # æ‰¹é‡ç¼–ç ï¼ˆä¸€æ¬¡æ€§å‘é‡åŒ–æ‰€æœ‰æ–‡æœ¬ï¼‰
    event_vecs = model_instance.encode(event_texts, normalize_embeddings=True, show_progress_bar=False)
    chart_vecs = model_instance.encode(chart_features, normalize_embeddings=True, show_progress_bar=False)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆevent_count Ã— feature_countï¼‰
    sim_matrix = np.dot(event_vecs, chart_vecs.T)
    
    # ä¸ºæ¯ä¸ªäº‹ä»¶æ‰¾åˆ°æœ€ä½³åŒ¹é…ç‰¹å¾
    for i, e in enumerate(events):
        best_idx = np.argmax(sim_matrix[i])
        best_sim = float(sim_matrix[i, best_idx])
        e["similarity"] = round(best_sim, 4)
        e["top_match_feature"] = chart_features[best_idx] if best_sim > 0 else None
    
    return events

def extract_features_from_chart(chart_text: str):
    patterns = {
        "æ¯ç¼˜æµ…": r"(æ¯|æ¯äº²|å¥³æ€§é•¿è¾ˆ).{0,4}(äº¡|å»ä¸–|ç¼˜è–„|ä¸åˆ©)",
        "å©šè¿Ÿ": r"(å©š|é…å¶|å¤«å¦»).{0,4}(æ™š|è¿Ÿ|éš¾|ç ´)",
        "äº‹ä¸šæ³¢æŠ˜": r"(äº‹|ä¸š|å·¥ä½œ|å®˜ç¦„).{0,4}(æ³¢æŠ˜|å¤šå˜|èµ·ä¼|ç ´)",
        "è´¢è¿å¥½": r"(è´¢|é’±|æ”¶å…¥).{0,4}(æ—º|ä½³|å¥½|ç¨³)",
        "å¥åº·å¼±": r"(ç–¾|ç—…|èº«|å¥åº·).{0,4}(å¼±|ä¸ä½³|å—æŸ)",
    }
    features = [k for k, p in patterns.items() if re.search(p, chart_text)]
    raw_terms = re.findall(r"[\u4e00-\u9fa5]{2,6}", chart_text)
    features.extend(list(set(raw_terms[:50])))
    return list(set(features))

# ----------------------------------------------------------
# æ™ºèƒ½æƒé‡å­¦ä¹ ç³»ç»Ÿ
# ----------------------------------------------------------
def update_event_weights(supabase_client, unmatched_events):
    """åŠ¨æ€è°ƒæ•´äººç”Ÿäº‹ä»¶æƒé‡ï¼ŒåŸºäºç›¸ä¼¼åº¦è¿›è¡Œæ™ºèƒ½å­¦ä¹ """
    if supabase_client is None:
        return
    
    for e in unmatched_events:
        new_weight = e["weight"]
        sim = e["similarity"]
        
        # æ™ºèƒ½æƒé‡è°ƒæ•´ç­–ç•¥
        if sim > 0.6:
            # ç›¸ä¼¼åº¦é«˜ä½†æœªåŒ¹é…ï¼Œå¢åŠ æƒé‡
            new_weight = min(e["weight"] + 0.1, 3.0)
        elif sim < 0.3:
            # ç›¸ä¼¼åº¦æä½ï¼Œé™ä½æƒé‡
            new_weight = max(e["weight"] - 0.05, 0.5)
        
        e["weight"] = new_weight
        
        try:
            supabase_client.table("life_event_weights").upsert({
                "event_desc": e["desc"],
                "weight": new_weight,
                "similarity": sim,
                "updated_at": datetime.now().isoformat()
            }, on_conflict="event_desc").execute()
            print(f"ğŸ“ˆ æƒé‡æ›´æ–°æˆ–æ–°å¢ï¼š{e['desc']} â†’ {new_weight:.2f}")
        except Exception as err:
            print(f"âš ï¸ æƒé‡ä¿å­˜å¤±è´¥ï¼š{e['desc']} | {err}")

def save_life_tags(supabase_client, user_id, life_tags):
    """ä¿å­˜æˆ–æ›´æ–°ç”¨æˆ·çš„ life_tagsï¼ˆäººç”Ÿæ ‡ç­¾ï¼‰"""
    if supabase_client is None:
        return
    
    try:
        # Supabase upsert() ä¼šè‡ªåŠ¨ä½¿ç”¨ user_id çš„ UNIQUE çº¦æŸè¿›è¡Œå†²çªæ£€æµ‹
        supabase_client.table("user_life_tags").upsert({
            "user_id": user_id,
            **life_tags,
            "updated_at": datetime.now().isoformat()
        }).execute()
        
        # æå–å…³é”®æ ‡ç­¾ä¿¡æ¯ç”¨äºæ—¥å¿—
        career = life_tags.get("career_type", "æœªçŸ¥èŒä¸š")
        marriage = life_tags.get("marriage_status", "æœªçŸ¥å©šå§»")
        print(f"ğŸ’¾ UPSERT ç”¨æˆ·æ ‡ç­¾ï¼š{user_id} â†’ {career}, {marriage}")
    except Exception as err:
        print(f"âš ï¸ ä¿å­˜ life_tags å¤±è´¥: {err}")

# ----------------------------------------------------------
# å•å‘½ç›˜éªŒè¯é€»è¾‘
# ----------------------------------------------------------
def verify_chart(user_id: str, chart_data: dict, life_data: dict):
    chart_text = " ".join([
        chart_data.get("notes", ""),
        chart_data.get("main_star", ""),
        chart_data.get("source", ""),
        chart_data.get("birth_datetime", "")
    ])
    features = extract_features_from_chart(chart_text)
    
    # âš¡ æ‰¹é‡å‘é‡åŒ–ï¼šä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰äº‹ä»¶ä¸ç‰¹å¾çš„ç›¸ä¼¼åº¦
    events = [ev.copy() for ev in life_data.get("events", [])]
    events = batch_similarity(model, events, features)
    
    matched, unmatched = [], []
    total_weight, gained_score = 0, 0

    # æ ¹æ®æ‰¹é‡è®¡ç®—çš„ç›¸ä¼¼åº¦è¿›è¡Œåˆ†ç±»
    for ev in events:
        weight = ev.get("weight", 1.0)
        total_weight += weight
        best_sim = ev.get("similarity", 0.0)

        if best_sim >= 0.6:
            matched.append(ev)
            gained_score += weight * best_sim
        else:
            unmatched.append(ev)

    score = round(gained_score / total_weight, 3) if total_weight else 0.0
    confidence = "é«˜" if score >= 0.85 else "ä¸­" if score >= 0.65 else "ä½"

    result = {
        "chart_id": chart_data.get("chart_id", "unknown"),
        "score": score,
        "confidence": confidence,
        "matched_keywords": list({ev["top_match_feature"] for ev in matched if ev.get("top_match_feature")}),
        "verified_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

    # âœ… æ™ºèƒ½æƒé‡å­¦ä¹ ï¼šæ ¹æ®æœªåŒ¹é…äº‹ä»¶çš„ç›¸ä¼¼åº¦åŠ¨æ€è°ƒæ•´æƒé‡
    unmatched_events = unmatched
    if unmatched_events:
        print(f"\nğŸ§  æ™ºèƒ½æƒé‡å­¦ä¹ ä¸­... ({len(unmatched_events)} ä¸ªæœªåŒ¹é…äº‹ä»¶)")
        update_event_weights(supabase, unmatched_events)
    
    # âœ… æ„å»ºå¹¶ä¿å­˜ç”¨æˆ· life_tags åˆ°æ•°æ®åº“
    life_tags = {
        "career_type": life_data.get("career_type", ""),
        "marriage_status": life_data.get("marriage_status", ""),
        "children": life_data.get("children", 0),
        "event_count": len(life_data.get("events", []))
    }
    print(f"\nğŸ’¾ ä¿å­˜ç”¨æˆ·äººç”Ÿæ ‡ç­¾...")
    save_life_tags(supabase, user_id, life_tags)

    # âœ… å†™å…¥éªŒè¯ç»“æœåˆ° Supabase
    if supabase:
        try:
            supabase.table("verified_charts").insert({
                "user_id": user_id,
                "chart_id": result["chart_id"],
                "score": result["score"],
                "confidence": result["confidence"],
                "matched_keywords": result["matched_keywords"],
                "verified_at": result["verified_at"]
            }).execute()
            print(f"ğŸ§¾ Saved verification record for {user_id} â†’ {result['chart_id']}")
        except Exception as e:
            print("âš ï¸ Supabase insert error:", e)

    return result

# ----------------------------------------------------------
# å¤šå‘½ç›˜ç»„åˆéªŒè¯ï¼ˆå¹¶å‘ä¼˜åŒ–ç‰ˆï¼‰
# ----------------------------------------------------------
def verify_multiple_charts(user_id: str, charts: list, life_data: dict):
    start_time = time.time()
    
    # âš¡ å¹¶å‘éªŒè¯ï¼šä½¿ç”¨ ThreadPoolExecutor å¹¶è¡Œå¤„ç†å¤šä¸ªå‘½ç›˜
    with ThreadPoolExecutor(max_workers=3) as executor:
        combo_results = list(executor.map(
            lambda ch: verify_chart(user_id, ch, life_data), 
            charts
        ))
    
    combo_results.sort(key=lambda x: x["score"], reverse=True)
    best = combo_results[0]["chart_id"] if combo_results else None

    output = {
        "user_id": user_id,
        "combos": combo_results,
        "best_match": best,
        "verified_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

    json.dump(output, open("./data/verified_birth_profiles.json", "w", encoding="utf-8"),
              ensure_ascii=False, indent=2)
    
    # æ€§èƒ½ç»Ÿè®¡
    elapsed = time.time() - start_time
    print(f"\nâš¡ æ‰¹é‡éªŒè¯å®Œæˆï¼Œæ€»è€—æ—¶ {elapsed:.2f}s")
    
    return output

# ----------------------------------------------------------
# ä¸»å…¥å£å‡½æ•°ï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ï¼‰
# ----------------------------------------------------------
def run_truechart_verifier(user_id="u_demo", supabase_client=None):
    """
    çœŸå‘½ç›˜éªŒè¯ä¸»å‡½æ•°
    å‚æ•°ï¼š
        user_id: ç”¨æˆ·ID
        supabase_client: Supabase å®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
    è¿”å›ï¼š
        éªŒè¯ç»“æœçš„ JSON å¯¹è±¡
    """
    os.makedirs("./data", exist_ok=True)
    
    # ç¤ºä¾‹æµ‹è¯•æ•°æ®
    chart_A = {
        "chart_id": "c_A",
        "source": "wenmo",
        "birth_datetime": "1975-05-10 23:10",
        "main_star": "å¤©åºœ",
        "notes": "å‘½å®«åœ¨å·³ï¼Œæ¯ç¼˜æµ…ï¼Œå©šè¿Ÿï¼Œäº‹ä¸šåˆæœ‰æ³¢æŠ˜åæˆï¼Œè´¢è¿ä¸­å¹´è§æ—º"
    }
    chart_B = {
        "chart_id": "c_B",
        "source": "wenmo",
        "birth_datetime": "1975-05-10 22:45",
        "main_star": "ç´«å¾®",
        "notes": "å‘½å®«åœ¨è¾°ï¼Œçˆ¶æ¯ç¼˜åšï¼Œå©šæ—©ä½†ç ´ï¼Œäº‹ä¸šç¨³ä¸­æœ‰è¿›"
    }
    chart_C = {
        "chart_id": "c_C",
        "source": "wenmo",
        "birth_datetime": "1975-05-10 23:30",
        "main_star": "æ­¦æ›²",
        "notes": "å‘½å®«åœ¨åˆï¼Œæ¯ç¼˜æµ…ï¼Œå©šå§»è¿Ÿæˆï¼Œäº‹ä¸šèµ·ä¼ï¼Œè´¢è¿åæ—º"
    }
    life_demo = {
        "career_type": "è®¾è®¡è¡Œä¸š",
        "marriage_status": "æ™šå©š",
        "children": 1,
        "events": [
            {"desc": "2003å¹´æ¯äº²å»ä¸–", "weight": 2.0},
            {"desc": "2006å¹´æµ·å¤–ç•™å­¦", "weight": 1.0},
            {"desc": "2010å¹´è·è®¾è®¡å¥–é¡¹", "weight": 1.5},
            {"desc": "å©šå§»æ™šæˆï¼Œå¦»å­å¹´é•¿å…«å²", "weight": 1.2}
        ]
    }
    
    charts = [chart_A, chart_B, chart_C]
    
    # ä½¿ç”¨ä¼ å…¥çš„ supabase å®¢æˆ·ç«¯æˆ–å…¨å±€çš„ supabase
    global supabase
    if supabase_client:
        supabase = supabase_client
    
    result = verify_multiple_charts(user_id, charts, life_demo)
    return result

# ----------------------------------------------------------
# æ‰‹åŠ¨æµ‹è¯•
# ----------------------------------------------------------
if __name__ == "__main__":
    result = run_truechart_verifier("u_demo")
    print(json.dumps(result, ensure_ascii=False, indent=2))
